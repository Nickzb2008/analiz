import tkinter as tk
from tkinter import ttk, messagebox, scrolledtext
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import tensorflow as tf
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, Callback
import os
import threading
import time
from datetime import datetime
import json
import sys
from io import StringIO
from utils.data_validator import DataValidator
from utils.file_selector import FileSelector
from utils.model_manager import ModelManager
from utils.data_processor import DataProcessor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error
# –í–∏–º–∫–Ω–µ–Ω–Ω—è –∑–∞–π–≤–∏—Ö –ø–æ–ø–µ—Ä–µ–¥–∂–µ–Ω—å
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # 0 = all, 1 = info, 2 = warnings, 3 = errors
import tensorflow as tf
tf.get_logger().setLevel('ERROR')


class TrainingProgressCallback(Callback):
    """–ö–∞—Å—Ç–æ–º–Ω–∏–π callback –¥–ª—è –≤—ñ–¥—Å—Ç–µ–∂–µ–Ω–Ω—è –ø—Ä–æ–≥—Ä–µ—Å—É –Ω–∞–≤—á–∞–Ω–Ω—è"""
    def __init__(self, status_callback, progress_callback, total_epochs, log_callback=None):
        super().__init__()
        self.status_callback = status_callback
        self.progress_callback = progress_callback
        self.total_epochs = total_epochs
        self.log_callback = log_callback
        self.best_epoch = 0
        self.best_loss = float('inf')
        self.start_time = time.time()
    
    def on_train_begin(self, logs=None):
        if self.log_callback:
            self.log_callback(f"üèÅ –ü–æ—á–∞—Ç–æ–∫ –Ω–∞–≤—á–∞–Ω–Ω—è. –ó–∞–ø–ª–∞–Ω–æ–≤–∞–Ω–æ –µ–ø–æ—Ö: {self.total_epochs}\n")
    
    def on_epoch_begin(self, epoch, logs=None):
        current_epoch = epoch + 1
        message = f"–ï–ø–æ—Ö–∞ {current_epoch}/{self.total_epochs}..."
        self.status_callback(message)
        if self.log_callback:
            self.log_callback(f"üîπ {message}\n")
    
    def on_epoch_end(self, epoch, logs=None):
        current_epoch = epoch + 1
        progress = (current_epoch / self.total_epochs) * 100
        self.progress_callback(min(progress, 100))
        
        # –í—ñ–¥—Å—Ç–µ–∂–µ–Ω–Ω—è –Ω–∞–π–∫—Ä–∞—â–æ—ó –µ–ø–æ—Ö–∏
        current_loss = logs.get('val_loss', float('inf'))
        if current_loss < self.best_loss:
            self.best_loss = current_loss
            self.best_epoch = current_epoch
            message = f"üéØ –ù–∞–π–∫—Ä–∞—â–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –Ω–∞ –µ–ø–æ—Å—ñ {self.best_epoch}: val_loss={current_loss:.6f}"
            self.status_callback(message)
            if self.log_callback:
                self.log_callback(f"‚úÖ {message}\n")
    
    def on_train_end(self, logs=None):
        training_time = time.time() - self.start_time
        if self.log_callback:
            self.log_callback(f"‚è±Ô∏è –ß–∞—Å –Ω–∞–≤—á–∞–Ω–Ω—è: {training_time:.1f} —Å–µ–∫—É–Ω–¥\n")
            self.log_callback(f"üèÜ –ù–∞–π–∫—Ä–∞—â–∞ –µ–ø–æ—Ö–∞: {self.best_epoch} (val_loss={self.best_loss:.6f})\n\n")

class TrainingNeuralNetworksTab:
    def __init__(self, parent, status_callback, progress_callback):
        self.parent = parent
        self.status_callback = status_callback
        self.progress_callback = progress_callback
        self.model_manager = ModelManager()
        self.current_training_thread = None
        self.training_stop_flag = False
        self.log_window = None
        self.log_text = None
        self.original_stdout = None
        self.original_stderr = None
        self.setup_ui()
    
    def setup_ui(self):
        """–ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è —ñ–Ω—Ç–µ—Ä—Ñ–µ–π—Å—É –Ω–∞–≤—á–∞–Ω–Ω—è –Ω–µ–π—Ä–æ–º–µ—Ä–µ–∂"""
        # –û—Å–Ω–æ–≤–Ω—ñ —Ñ—Ä–µ–π–º–∏
        main_frame = ttk.Frame(self.parent)
        main_frame.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)
        
        # –õ—ñ–≤–∏–π —Ñ—Ä–µ–π–º - –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –Ω–∞–≤—á–∞–Ω–Ω—è –∑ —Ñ—ñ–∫—Å–æ–≤–∞–Ω–æ—é —à–∏—Ä–∏–Ω–æ—é
        left_frame_container = ttk.Frame(main_frame, width=300)
        left_frame_container.pack(side=tk.LEFT, fill=tk.Y, padx=5, pady=5)
        left_frame_container.pack_propagate(False)  # –§—ñ–∫—Å—É—î–º–æ —à–∏—Ä–∏–Ω—É
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ Canvas —ñ Scrollbar –¥–ª—è –ø—Ä–æ–∫—Ä—É—Ç–∫–∏
        canvas = tk.Canvas(left_frame_container, highlightthickness=0)
        scrollbar = ttk.Scrollbar(left_frame_container, orient=tk.VERTICAL, command=canvas.yview)
        scrollable_frame = ttk.Frame(canvas)
        
        scrollable_frame.bind(
            "<Configure>",
            lambda e: canvas.configure(scrollregion=canvas.bbox("all"))
        )
        
        canvas.create_window((0, 0), window=scrollable_frame, anchor="nw")
        canvas.configure(yscrollcommand=scrollbar.set)
        
        # –î–æ–¥–∞—î–º–æ –ø—Ä–æ–∫—Ä—É—Ç–∫—É –º–∏—à–µ—é
        def _on_mousewheel(event):
            canvas.yview_scroll(int(-1 * (event.delta / 120)), "units")
        
        canvas.bind("<Enter>", lambda e: canvas.bind_all("<MouseWheel>", _on_mousewheel))
        canvas.bind("<Leave>", lambda e: canvas.unbind_all("<MouseWheel>"))
        
        canvas.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)
        scrollbar.pack(side=tk.RIGHT, fill=tk.Y)
        
        # –¶–µ–Ω—Ç—Ä–∞–ª—å–Ω–∏–π —Ñ—Ä–µ–π–º - –≤–∏–±—ñ—Ä –¥–∞–Ω–∏—Ö
        center_frame = ttk.LabelFrame(main_frame, text="–í–∏–±—ñ—Ä –¥–∞–Ω–∏—Ö")
        center_frame.pack(side=tk.LEFT, fill=tk.BOTH, expand=True, padx=5, pady=5)
        
        # –ü—Ä–∞–≤–∏–π —Ñ—Ä–µ–π–º - –≥—Ä–∞—Ñ—ñ–∫ –Ω–∞–≤—á–∞–Ω–Ω—è
        right_frame = ttk.LabelFrame(main_frame, text="–ì—Ä–∞—Ñ—ñ–∫ –Ω–∞–≤—á–∞–Ω–Ω—è")
        right_frame.pack(side=tk.RIGHT, fill=tk.BOTH, expand=True, padx=5, pady=5)
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ –Ω–∞–≤—á–∞–Ω–Ω—è (—Ç–µ–ø–µ—Ä —É scrollable_frame)
        ttk.Label(scrollable_frame, text="–¢–∏–ø –Ω–∞–≤—á–∞–Ω–Ω—è:", font=('Arial', 10, 'bold')).pack(pady=(10, 5), anchor=tk.W, padx=10)
        self.training_type_var = tk.StringVar(value="basic")
        training_types = [("–ë–∞–∑–æ–≤–µ", "basic"), ("–†–æ–∑—à–∏—Ä–µ–Ω–µ", "advanced"), ("–ï–∫—Å–ø–µ—Ä—Ç–Ω–µ", "expert")]
        for text, value in training_types:
            ttk.Radiobutton(scrollable_frame, text=text, variable=self.training_type_var, value=value).pack(anchor=tk.W, padx=20)
        
        ttk.Label(scrollable_frame, text="–û—Å–Ω–æ–≤–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏:", font=('Arial', 10, 'bold')).pack(pady=(15, 5), anchor=tk.W, padx=10)
        
        ttk.Label(scrollable_frame, text="–ö—ñ–ª—å–∫—ñ—Å—Ç—å –µ–ø–æ—Ö:").pack(anchor=tk.W, padx=20)
        self.epochs_var = tk.IntVar(value=50)
        ttk.Entry(scrollable_frame, textvariable=self.epochs_var, width=15).pack(anchor=tk.W, padx=20, pady=(0, 5))
        
        ttk.Label(scrollable_frame, text="–†–æ–∑–º—ñ—Ä –±–∞—Ç—á–∞:").pack(anchor=tk.W, padx=20)
        self.batch_size_var = tk.IntVar(value=32)
        ttk.Entry(scrollable_frame, textvariable=self.batch_size_var, width=15).pack(anchor=tk.W, padx=20, pady=(0, 5))
        
        ttk.Label(scrollable_frame, text="–†–æ–∑–º—ñ—Ä –≤—ñ–∫–Ω–∞:").pack(anchor=tk.W, padx=20)
        self.lookback_var = tk.IntVar(value=60)
        ttk.Entry(scrollable_frame, textvariable=self.lookback_var, width=15).pack(anchor=tk.W, padx=20, pady=(0, 5))
        
        ttk.Label(scrollable_frame, text="–†–æ–∑–º—ñ—Ä —Ç–µ—Å—Ç–æ–≤–æ—ó –≤–∏–±—ñ—Ä–∫–∏ (%):").pack(anchor=tk.W, padx=20)
        self.test_size_var = tk.DoubleVar(value=0.2)
        ttk.Entry(scrollable_frame, textvariable=self.test_size_var, width=15).pack(anchor=tk.W, padx=20, pady=(0, 5))
        
        ttk.Label(scrollable_frame, text="Learning Rate:").pack(anchor=tk.W, padx=20)
        self.lr_var = tk.DoubleVar(value=0.001)
        ttk.Entry(scrollable_frame, textvariable=self.lr_var, width=15).pack(anchor=tk.W, padx=20, pady=(0, 10))
        
        ttk.Label(scrollable_frame, text="–ü–∞—Ä–∞–º–µ—Ç—Ä–∏ —Ä–∞–Ω–Ω—å–æ—ó –∑—É–ø–∏–Ω–∫–∏:", font=('Arial', 10, 'bold')).pack(pady=(10, 5), anchor=tk.W, padx=10)
        
        ttk.Label(scrollable_frame, text="Patience (—Ç–µ—Ä–ø—ñ–Ω–Ω—è):").pack(anchor=tk.W, padx=20)
        self.patience_var = tk.IntVar(value=10)
        ttk.Entry(scrollable_frame, textvariable=self.patience_var, width=15).pack(anchor=tk.W, padx=20, pady=(0, 5))
        
        ttk.Label(scrollable_frame, text="–ú—ñ–Ω—ñ–º–∞–ª—å–Ω–µ –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è:").pack(anchor=tk.W, padx=20)
        self.min_delta_var = tk.DoubleVar(value=0.0001)
        ttk.Entry(scrollable_frame, textvariable=self.min_delta_var, width=15).pack(anchor=tk.W, padx=20, pady=(0, 10))
        
        # –î–æ–¥–∞—Ç–∫–æ–≤—ñ –æ–ø—Ü—ñ—ó
        ttk.Label(scrollable_frame, text="–î–æ–¥–∞—Ç–∫–æ–≤—ñ –æ–ø—Ü—ñ—ó:", font=('Arial', 10, 'bold')).pack(pady=(10, 5), anchor=tk.W, padx=10)
        
        self.use_technical_var = tk.BooleanVar(value=True)
        ttk.Checkbutton(scrollable_frame, text="–¢–µ—Ö–Ω—ñ—á–Ω—ñ —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä–∏", 
                    variable=self.use_technical_var).pack(anchor=tk.W, padx=20)
        
        self.use_time_features_var = tk.BooleanVar(value=True)
        ttk.Checkbutton(scrollable_frame, text="–ß–∞—Å–æ–≤—ñ –æ–∑–Ω–∞–∫–∏", 
                    variable=self.use_time_features_var).pack(anchor=tk.W, padx=20)
        
        ttk.Label(scrollable_frame, text="–ü–æ–∫—Ä–∞—â–µ–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –Ω–∞–≤—á–∞–Ω–Ω—è:", font=('Arial', 10, 'bold')).pack(pady=(15, 5), anchor=tk.W, padx=10)

        # Dropout rates
        ttk.Label(scrollable_frame, text="Dropout Rate (–ø–æ—á–∞—Ç–∫–æ–≤–∏–π):").pack(anchor=tk.W, padx=20)
        self.dropout_start_var = tk.DoubleVar(value=0.2)
        ttk.Entry(scrollable_frame, textvariable=self.dropout_start_var, width=20).pack(anchor=tk.W, padx=20, pady=(0, 5))

        ttk.Label(scrollable_frame, text="Dropout Rate (–∫—ñ–Ω—Ü–µ–≤–∏–π):").pack(anchor=tk.W, padx=20)
        self.dropout_end_var = tk.DoubleVar(value=0.4)
        ttk.Entry(scrollable_frame, textvariable=self.dropout_end_var, width=20).pack(anchor=tk.W, padx=20, pady=(0, 5))

        # –ö—ñ–ª—å–∫—ñ—Å—Ç—å LSTM —à–∞—Ä—ñ–≤
        ttk.Label(scrollable_frame, text="–ö—ñ–ª—å–∫—ñ—Å—Ç—å LSTM —à–∞—Ä—ñ–≤:").pack(anchor=tk.W, padx=20)
        self.lstm_layers_var = tk.IntVar(value=2)
        ttk.Entry(scrollable_frame, textvariable=self.lstm_layers_var, width=20).pack(anchor=tk.W, padx=20, pady=(0, 5))

        # –†–æ–∑–º—ñ—Ä LSTM —é–Ω—ñ—Ç—ñ–≤
        ttk.Label(scrollable_frame, text="–†–æ–∑–º—ñ—Ä LSTM —é–Ω—ñ—Ç—ñ–≤ (–ø–æ—á–∞—Ç–∫–æ–≤–∏–π):").pack(anchor=tk.W, padx=20)
        self.lstm_units_start_var = tk.IntVar(value=64)
        ttk.Entry(scrollable_frame, textvariable=self.lstm_units_start_var, width=20).pack(anchor=tk.W, padx=20, pady=(0, 5))

        ttk.Label(scrollable_frame, text="–†–æ–∑–º—ñ—Ä LSTM —é–Ω—ñ—Ç—ñ–≤ (–∫—ñ–Ω—Ü–µ–≤–∏–π):").pack(anchor=tk.W, padx=20)
        self.lstm_units_end_var = tk.IntVar(value=32)
        ttk.Entry(scrollable_frame, textvariable=self.lstm_units_end_var, width=20).pack(anchor=tk.W, padx=20, pady=(0, 5))

        # –î–æ–¥–∞—Ç–∫–æ–≤—ñ —Ç–µ—Ö–Ω—ñ—á–Ω—ñ —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä–∏
        ttk.Label(scrollable_frame, text="–†–æ–∑—à–∏—Ä–µ–Ω—ñ —Ç–µ—Ö–Ω—ñ—á–Ω—ñ —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä–∏:", font=('Arial', 10, 'bold')).pack(pady=(15, 5), anchor=tk.W, padx=10)

        self.use_rsi_var = tk.BooleanVar(value=True)
        ttk.Checkbutton(scrollable_frame, text="RSI (Relative Strength Index)", 
                    variable=self.use_rsi_var, width=25).pack(anchor=tk.W, padx=20)

        self.use_macd_var = tk.BooleanVar(value=True)
        ttk.Checkbutton(scrollable_frame, text="MACD", 
                    variable=self.use_macd_var, width=25).pack(anchor=tk.W, padx=20)

        self.use_bollinger_var = tk.BooleanVar(value=True)
        ttk.Checkbutton(scrollable_frame, text="–°–º—É–≥–∏ –ë–æ–ª–ª—ñ–Ω–¥–∂–µ—Ä–∞", 
                    variable=self.use_bollinger_var, width=25).pack(anchor=tk.W, padx=20)

        self.use_atr_var = tk.BooleanVar(value=False)
        ttk.Checkbutton(scrollable_frame, text="ATR (Average True Range)", 
                    variable=self.use_atr_var, width=25).pack(anchor=tk.W, padx=20)

        self.use_volume_indicators_var = tk.BooleanVar(value=False)
        ttk.Checkbutton(scrollable_frame, text="–í–æ–ª—å—é–º —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä–∏", 
                    variable=self.use_volume_indicators_var, width=25).pack(anchor=tk.W, padx=20)

        # –ü–æ–∫—Ä–∞—â–µ–Ω—ñ —á–∞—Å–æ–≤—ñ –æ–∑–Ω–∞–∫–∏
        ttk.Label(scrollable_frame, text="–ü–æ–∫—Ä–∞—â–µ–Ω—ñ —á–∞—Å–æ–≤—ñ –æ–∑–Ω–∞–∫–∏:", font=('Arial', 10, 'bold')).pack(pady=(15, 5), anchor=tk.W, padx=10)

        self.use_seasonal_features_var = tk.BooleanVar(value=False)
        ttk.Checkbutton(scrollable_frame, text="–°–µ–∑–æ–Ω–Ω—ñ—Å—Ç—å (—Ä—ñ–∫/–∫–≤–∞—Ä—Ç–∞–ª)", 
                    variable=self.use_seasonal_features_var, width=25).pack(anchor=tk.W, padx=20)

        self.use_cyclical_features_var = tk.BooleanVar(value=False)
        ttk.Checkbutton(scrollable_frame, text="–¶–∏–∫–ª—ñ—á–Ω—ñ –æ–∑–Ω–∞–∫–∏ (sin/cos)", 
                    variable=self.use_cyclical_features_var, width=25).pack(anchor=tk.W, padx=20)

        # –ú–µ—Ç–æ–¥–∏ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—ó
        ttk.Label(scrollable_frame, text="–ú–µ—Ç–æ–¥–∏ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—ó:", font=('Arial', 10, 'bold')).pack(pady=(15, 5), anchor=tk.W, padx=10)

        self.use_batch_norm_var = tk.BooleanVar(value=False)
        ttk.Checkbutton(scrollable_frame, text="Batch Normalization", 
                    variable=self.use_batch_norm_var, width=25).pack(anchor=tk.W, padx=20)

        self.use_l2_reg_var = tk.BooleanVar(value=False)
        ttk.Checkbutton(scrollable_frame, text="L2 Regularization", 
                    variable=self.use_l2_reg_var, width=25).pack(anchor=tk.W, padx=20)

        self.use_early_stopping_var = tk.BooleanVar(value=True)
        ttk.Checkbutton(scrollable_frame, text="–†–∞–Ω–Ω—è –∑—É–ø–∏–Ω–∫–∞", 
                    variable=self.use_early_stopping_var, width=25).pack(anchor=tk.W, padx=20)
        
        
        ttk.Label(scrollable_frame, text="–¢–æ—Ä–≥–æ–≤—ñ —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä–∏:", font=('Arial', 10, 'bold')).pack(pady=(10, 5), anchor=tk.W, padx=10)

        self.use_volatility_indicators_var = tk.BooleanVar(value=True)
        ttk.Checkbutton(scrollable_frame, text="–í–æ–ª–∞—Ç–∏–ª—å–Ω—ñ—Å—Ç—å (ATR, Volatility)", 
                    variable=self.use_volatility_indicators_var).pack(anchor=tk.W, padx=20)

        self.use_momentum_indicators_var = tk.BooleanVar(value=True)
        ttk.Checkbutton(scrollable_frame, text="Momentum (RSI, Stochastic, MACD)", 
                    variable=self.use_momentum_indicators_var).pack(anchor=tk.W, padx=20)

        self.use_volume_indicators_var = tk.BooleanVar(value=True)
        ttk.Checkbutton(scrollable_frame, text="–û–±'—î–º–Ω—ñ —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä–∏ (OBV, Volume)", 
                    variable=self.use_volume_indicators_var).pack(anchor=tk.W, padx=20)

        self.use_market_indicators_var = tk.BooleanVar(value=False)
        ttk.Checkbutton(scrollable_frame, text="–†–∏–Ω–∫–æ–≤—ñ –ø–æ–∫–∞–∑–Ω–∏–∫–∏ (BTC correlation)", 
                    variable=self.use_market_indicators_var).pack(anchor=tk.W, padx=20)

        self.use_risk_metrics_var = tk.BooleanVar(value=False)
        ttk.Checkbutton(scrollable_frame, text="–ú–µ—Ç—Ä–∏–∫–∏ —Ä–∏–∑–∏–∫—É (VaR, Drawdown)", 
                    variable=self.use_risk_metrics_var).pack(anchor=tk.W, padx=20)

        # –¶—ñ–ª—å–æ–≤—ñ –∑–º—ñ–Ω–Ω—ñ –¥–ª—è —Ç–æ—Ä–≥—ñ–≤–ª—ñ
        ttk.Label(scrollable_frame, text="–¶—ñ–ª—å–æ–≤—ñ –∑–º—ñ–Ω–Ω—ñ:", font=('Arial', 10, 'bold')).pack(pady=(10, 5), anchor=tk.W, padx=10)

        self.forecast_horizon_var = tk.StringVar(value="5")
        ttk.Label(scrollable_frame, text="–ì–æ—Ä–∏–∑–æ–Ω—Ç –ø—Ä–æ–≥–Ω–æ–∑—É (–¥–Ω—ñ–≤):").pack(anchor=tk.W, padx=20)
        ttk.Entry(scrollable_frame, textvariable=self.forecast_horizon_var, width=10).pack(anchor=tk.W, padx=20, pady=(0, 10))

        self.use_price_target_var = tk.BooleanVar(value=True)
        ttk.Checkbutton(scrollable_frame, text="–ü—Ä–æ–≥–Ω–æ–∑ —Ü—ñ–Ω–∏", 
                    variable=self.use_price_target_var).pack(anchor=tk.W, padx=20)

        self.use_return_target_var = tk.BooleanVar(value=True)
        ttk.Checkbutton(scrollable_frame, text="–ü—Ä–æ–≥–Ω–æ–∑ –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—ñ", 
                    variable=self.use_return_target_var).pack(anchor=tk.W, padx=20)

        self.use_signal_target_var = tk.BooleanVar(value=False)
        ttk.Checkbutton(scrollable_frame, text="–¢–æ—Ä–≥–æ–≤—ñ —Å–∏–≥–Ω–∞–ª–∏", 
                    variable=self.use_signal_target_var).pack(anchor=tk.W, padx=20)
        
        
            # –î–æ–¥–∞—î–º–æ –Ω–æ–≤—É —Å–µ–∫—Ü—ñ—é –¥–ª—è –≤–∏–±–æ—Ä—É –æ–∑–Ω–∞–∫
        ttk.Label(scrollable_frame, text="–í–∏–±—ñ—Ä –æ–∑–Ω–∞–∫ –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è:", 
                font=('Arial', 10, 'bold')).pack(pady=(15, 5), anchor=tk.W, padx=10)
        
        # –û—Å–Ω–æ–≤–Ω—ñ —Ü—ñ–Ω–æ–≤—ñ –æ–∑–Ω–∞–∫–∏
        self.use_close_var = tk.BooleanVar(value=True)
        ttk.Checkbutton(scrollable_frame, text="–¶—ñ–Ω–∞ –∑–∞–∫—Ä–∏—Ç—Ç—è (Close)", 
                        variable=self.use_close_var).pack(anchor=tk.W, padx=20)
        
        self.use_high_low_var = tk.BooleanVar(value=False)
        ttk.Checkbutton(scrollable_frame, text="High/Low —Ü—ñ–Ω–∏", 
                        variable=self.use_high_low_var).pack(anchor=tk.W, padx=20)
        
        self.use_open_var = tk.BooleanVar(value=False)
        ttk.Checkbutton(scrollable_frame, text="–¶—ñ–Ω–∞ –≤—ñ–¥–∫—Ä–∏—Ç—Ç—è (Open)", 
                        variable=self.use_open_var).pack(anchor=tk.W, padx=20)
        
        self.use_volume_var = tk.BooleanVar(value=False)
        ttk.Checkbutton(scrollable_frame, text="–û–±'—î–º —Ç–æ—Ä–≥—ñ–≤ (Volume)", 
                        variable=self.use_volume_var).pack(anchor=tk.W, padx=20)
        
        # –¢–µ—Ö–Ω—ñ—á–Ω—ñ —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä–∏
        ttk.Label(scrollable_frame, text="–¢–µ—Ö–Ω—ñ—á–Ω—ñ —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä–∏:", 
                font=('Arial', 9, 'bold')).pack(pady=(10, 5), anchor=tk.W, padx=20)
        
        self.use_returns_var = tk.BooleanVar(value=True)
        ttk.Checkbutton(scrollable_frame, text="–î–µ–Ω–Ω–∞ –¥–æ—Ö–æ–¥–Ω—ñ—Å—Ç—å (Returns)", 
                        variable=self.use_returns_var).pack(anchor=tk.W, padx=30)
        
        self.use_ma_var = tk.BooleanVar(value=True)
        ttk.Checkbutton(scrollable_frame, text="–ö–æ–≤–∑–Ω—ñ —Å–µ—Ä–µ–¥–Ω—ñ (MA)", 
                        variable=self.use_ma_var).pack(anchor=tk.W, padx=30)
        
        self.use_volatility_var = tk.BooleanVar(value=True)
        ttk.Checkbutton(scrollable_frame, text="–í–æ–ª–∞—Ç–∏–ª—å–Ω—ñ—Å—Ç—å", 
                        variable=self.use_volatility_var).pack(anchor=tk.W, padx=30)
        
        self.use_rsi_var = tk.BooleanVar(value=False)
        ttk.Checkbutton(scrollable_frame, text="RSI", 
                        variable=self.use_rsi_var).pack(anchor=tk.W, padx=30)
        
        self.use_macd_var = tk.BooleanVar(value=False)
        ttk.Checkbutton(scrollable_frame, text="MACD", 
                        variable=self.use_macd_var).pack(anchor=tk.W, padx=30)
        
        # –ß–∞—Å–æ–≤—ñ –æ–∑–Ω–∞–∫–∏
        ttk.Label(scrollable_frame, text="–ß–∞—Å–æ–≤—ñ –æ–∑–Ω–∞–∫–∏:", 
                font=('Arial', 9, 'bold')).pack(pady=(10, 5), anchor=tk.W, padx=20)
        
        self.use_day_of_week_var = tk.BooleanVar(value=True)
        ttk.Checkbutton(scrollable_frame, text="–î–µ–Ω—å —Ç–∏–∂–Ω—è", 
                        variable=self.use_day_of_week_var).pack(anchor=tk.W, padx=30)
        
        self.use_month_var = tk.BooleanVar(value=True)
        ttk.Checkbutton(scrollable_frame, text="–ú—ñ—Å—è—Ü—å", 
                        variable=self.use_month_var).pack(anchor=tk.W, padx=30)
        
        self.use_quarter_var = tk.BooleanVar(value=False)
        ttk.Checkbutton(scrollable_frame, text="–ö–≤–∞—Ä—Ç–∞–ª", 
                        variable=self.use_quarter_var).pack(anchor=tk.W, padx=30)
        


                # –î–æ–¥–∞—î–º–æ –ø—ñ—Å–ª—è —á–µ–∫–±–æ–∫—Å—ñ–≤ —É setup_ui
        ttk.Label(scrollable_frame, text="–ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∏–π –≤–∏–±—ñ—Ä:", 
                font=('Arial', 9, 'bold')).pack(pady=(10, 5), anchor=tk.W, padx=20)

        button_frame = ttk.Frame(scrollable_frame)
        button_frame.pack(fill=tk.X, padx=20, pady=(0, 10))

        ttk.Button(button_frame, text="–ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∏–π", 
                command=lambda: self.auto_select_features("minimal")).pack(side=tk.LEFT, padx=2)
        ttk.Button(button_frame, text="–¢–æ—Ä–≥—ñ–≤–µ–ª—å–Ω–∏–π", 
                command=lambda: self.auto_select_features("trading")).pack(side=tk.LEFT, padx=2)
        ttk.Button(button_frame, text="–ü–æ–≤–Ω–∏–π", 
                command=lambda: self.auto_select_features("comprehensive")).pack(side=tk.LEFT, padx=2)

        ttk.Label(scrollable_frame, text="–ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∏–π –ø—ñ–¥–±—ñ—Ä LR:").pack(anchor=tk.W, padx=20)
        self.auto_lr_var = tk.BooleanVar(value=True)
        ttk.Checkbutton(scrollable_frame, text="–ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∏–π –ø—ñ–¥–±—ñ—Ä Learning Rate", 
                    variable=self.auto_lr_var).pack(anchor=tk.W, padx=20)

        ttk.Label(scrollable_frame, text="–ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∞ –∫–æ—Ä–µ–ª—è—Ü—ñ—è:").pack(anchor=tk.W, padx=20)
        self.min_correlation_var = tk.DoubleVar(value=0.15)
        ttk.Entry(scrollable_frame, textvariable=self.min_correlation_var, width=10).pack(anchor=tk.W, padx=20)
        
        # –ß–µ–∫–±–æ–∫—Å –¥–ª—è –ø–æ–∫–∞–∑—É –ª–æ–≥—É
        self.show_log_var = tk.BooleanVar(value=False)
        ttk.Checkbutton(scrollable_frame, text="–ü–æ–∫–∞–∑—É–≤–∞—Ç–∏ –ª–æ–≥ –Ω–∞–≤—á–∞–Ω–Ω—è", 
                    variable=self.show_log_var, command=self.toggle_log_window).pack(anchor=tk.W, padx=20, pady=(0, 15))
        
        
        
        
        # –ö–Ω–æ–ø–∫–∏ —É–ø—Ä–∞–≤–ª—ñ–Ω–Ω—è –¥–∞–Ω–∏–º–∏
        ttk.Label(scrollable_frame, text="–£–ø—Ä–∞–≤–ª—ñ–Ω–Ω—è –¥–∞–Ω–∏–º–∏:", font=('Arial', 10, 'bold')).pack(pady=(10, 5), anchor=tk.W, padx=10)
        
        ttk.Button(scrollable_frame, text="–û–Ω–æ–≤–∏—Ç–∏ —Å–ø–∏—Å–æ–∫", command=self.refresh_data_list, width=20).pack(pady=5, padx=20)
        ttk.Button(scrollable_frame, text="–í–∏–±—Ä–∞—Ç–∏ –≤—Å—ñ", command=self.select_all, width=20).pack(pady=2, padx=20)
        ttk.Button(scrollable_frame, text="–°–∫–∞—Å—É–≤–∞—Ç–∏ –≤–∏–±—ñ—Ä", command=self.deselect_all, width=20).pack(pady=2, padx=20)
        
        ttk.Label(scrollable_frame, text="–ù–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª–µ–π:", font=('Arial', 10, 'bold')).pack(pady=(15, 5), anchor=tk.W, padx=10)
        
        ttk.Button(scrollable_frame, text="–ù–∞–≤—á–∏—Ç–∏ –æ–±—Ä–∞–Ω—ñ", command=self.train_selected, width=20).pack(pady=5, padx=20)
        ttk.Button(scrollable_frame, text="–ù–∞–≤—á–∏—Ç–∏ –≤—Å—ñ", command=self.train_all, width=20).pack(pady=2, padx=20)
        ttk.Button(scrollable_frame, text="–ó—É–ø–∏–Ω–∏—Ç–∏ –Ω–∞–≤—á–∞–Ω–Ω—è", command=self.stop_training, width=20).pack(pady=2, padx=20)
        ttk.Button(scrollable_frame, text="–û—á–∏—Å—Ç–∏—Ç–∏ –º–æ–¥–µ–ª—ñ", command=self.cleanup_models, width=20).pack(pady=5, padx=20)
        
        # –í–∏–±—ñ—Ä –¥–∞–Ω–∏—Ö
        tree_frame = ttk.Frame(center_frame)
        tree_frame.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)
        
        self.data_tree = ttk.Treeview(tree_frame, columns=('Symbol', 'Records', 'Last Update'), 
                                    show='headings', height=15, selectmode='extended')
        self.data_tree.heading('Symbol', text='–ö—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∞')
        self.data_tree.heading('Records', text='–ó–∞–ø–∏—Å—ñ–≤')
        self.data_tree.heading('Last Update', text='–û–Ω–æ–≤–ª–µ–Ω–æ')
        
        self.data_tree.column('Symbol', width=100)
        self.data_tree.column('Records', width=80)
        self.data_tree.column('Last Update', width=100)
        
        tree_scrollbar = ttk.Scrollbar(tree_frame, orient=tk.VERTICAL, command=self.data_tree.yview)
        self.data_tree.configure(yscrollcommand=tree_scrollbar.set)
        
        # –î–æ–¥–∞—î–º–æ –ø—Ä–æ–∫—Ä—É—Ç–∫—É –º–∏—à–µ—é –¥–ª—è Treeview
        def _on_tree_mousewheel(event):
            self.data_tree.yview_scroll(int(-1 * (event.delta / 120)), "units")
        
        self.data_tree.bind("<Enter>", lambda e: self.data_tree.bind_all("<MouseWheel>", _on_tree_mousewheel))
        self.data_tree.bind("<Leave>", lambda e: self.data_tree.unbind_all("<MouseWheel>"))
        
        self.data_tree.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)
        tree_scrollbar.pack(side=tk.RIGHT, fill=tk.Y)
        
        # –ì—Ä–∞—Ñ—ñ–∫ –Ω–∞–≤—á–∞–Ω–Ω—è
        self.fig, self.ax = plt.subplots(figsize=(8, 6))
        self.canvas = FigureCanvasTkAgg(self.fig, right_frame)
        self.canvas.get_tk_widget().pack(fill=tk.BOTH, expand=True, padx=5, pady=5)
        
        # –Ü–Ω—Ñ–æ—Ä–º–∞—Ü—ñ–π–Ω–∏–π —Ç–µ–∫—Å—Ç
        info_frame = ttk.LabelFrame(right_frame, text="–Ü–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –ø—Ä–æ –Ω–∞–≤—á–∞–Ω–Ω—è")
        info_frame.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)
        
        self.info_text = scrolledtext.ScrolledText(info_frame, height=10, width=50)
        self.info_text.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)
        
        # –°—Ç–∞—Ç—É—Å–Ω–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è
        status_frame = ttk.Frame(right_frame)
        status_frame.pack(fill=tk.X, padx=5, pady=5)
        
        ttk.Label(status_frame, text="–°—Ç–∞—Ç—É—Å:").pack(side=tk.LEFT)
        self.status_label = ttk.Label(status_frame, text="–ì–æ—Ç–æ–≤–∏–π –¥–æ –Ω–∞–≤—á–∞–Ω–Ω—è", foreground="green")
        self.status_label.pack(side=tk.LEFT, padx=5)
        
        self.progress_bar = ttk.Progressbar(status_frame, mode='determinate')
        self.progress_bar.pack(side=tk.RIGHT, fill=tk.X, expand=True, padx=5)
        
        # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ —Å–ø–∏—Å–æ–∫ –¥–∞–Ω–∏—Ö
        self.refresh_data_list()
        
        # –û–Ω–æ–≤–ª—é—î–º–æ –æ–±–ª–∞—Å—Ç—å –ø—Ä–æ–∫—Ä—É—Ç–∫–∏
        self.parent.after(100, self.update_scroll_region)

    def update_scroll_region(self):
        """–û–Ω–æ–≤–ª—é—î –æ–±–ª–∞—Å—Ç—å –ø—Ä–æ–∫—Ä—É—Ç–∫–∏ –ø—ñ—Å–ª—è –¥–æ–¥–∞–≤–∞–Ω–Ω—è –µ–ª–µ–º–µ–Ω—Ç—ñ–≤"""
        if hasattr(self, 'canvas') and hasattr(self.canvas, 'configure'):
            self.canvas.configure(scrollregion=self.canvas.bbox("all"))
    
    def refresh_data_list(self):
        """–û–Ω–æ–≤–ª—é—î —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω–∏—Ö –¥–∞–Ω–∏—Ö"""
        self.data_tree.delete(*self.data_tree.get_children())
        files = FileSelector.get_sorted_files()
        
        for file in files:
            try:
                symbol = file.replace('_data.csv', '')
                file_path = f'data/{file}'
                
                if os.path.exists(file_path):
                    # –û—Ç—Ä–∏–º—É—î–º–æ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Ä—è–¥–∫—ñ–≤
                    df = pd.read_csv(file_path)
                    records = len(df)
                    
                    # –û—Ç—Ä–∏–º—É—î–º–æ –¥–∞—Ç—É –æ—Å—Ç–∞–Ω–Ω—å–æ–≥–æ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è
                    mod_time = os.path.getmtime(file_path)
                    last_update = datetime.fromtimestamp(mod_time).strftime('%Y-%m-%d')
                    
                    self.data_tree.insert('', 'end', values=(symbol, records, last_update))
            except Exception as e:
                self.status_callback(f"–ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è {file}: {str(e)}")
    
    def select_all(self):
        """–í–∏–±–∏—Ä–∞—î –≤—Å—ñ –µ–ª–µ–º–µ–Ω—Ç–∏"""
        for item in self.data_tree.get_children():
            self.data_tree.selection_add(item)
    
    def deselect_all(self):
        """–°–∫–∞—Å–æ–≤—É—î –≤–∏–±—ñ—Ä –≤—Å—ñ—Ö –µ–ª–µ–º–µ–Ω—Ç—ñ–≤"""
        self.data_tree.selection_remove(self.data_tree.selection())
    
    def get_selected_symbols(self):
        """–ü–æ–≤–µ—Ä—Ç–∞—î —Å–ø–∏—Å–æ–∫ –≤–∏–±—Ä–∞–Ω–∏—Ö —Å–∏–º–≤–æ–ª—ñ–≤"""
        selected_items = self.data_tree.selection()
        return [self.data_tree.item(item, 'values')[0] for item in selected_items]
    
    def train_selected(self):
        """–ù–∞–≤—á–∞—î –≤–∏–±—Ä–∞–Ω—ñ –º–æ–¥–µ–ª—ñ"""
        selected_symbols = self.get_selected_symbols()
        if not selected_symbols:
            messagebox.showwarning("–£–≤–∞–≥–∞", "–û–±–µ—Ä—ñ—Ç—å —Ö–æ—á–∞ –± –æ–¥–∏–Ω —Å–∏–º–≤–æ–ª –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è")
            return
        self.start_training(selected_symbols)
    
    def train_all(self):
        """–ù–∞–≤—á–∞—î –≤—Å—ñ –º–æ–¥–µ–ª—ñ"""
        all_symbols = [self.data_tree.item(item, 'values')[0] for item in self.data_tree.get_children()]
        if not all_symbols:
            messagebox.showwarning("–£–≤–∞–≥–∞", "–ù–µ–º–∞—î –¥–æ—Å—Ç—É–ø–Ω–∏—Ö —Å–∏–º–≤–æ–ª—ñ–≤ –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è")
            return
        self.start_training(all_symbols)
    
    def start_training(self, symbols):
        """–ó–∞–ø—É—Å–∫ –Ω–∞–≤—á–∞–Ω–Ω—è"""
        if self.current_training_thread and self.current_training_thread.is_alive():
            messagebox.showwarning("–£–≤–∞–≥–∞", "–ù–∞–≤—á–∞–Ω–Ω—è –≤–∂–µ –≤–∏–∫–æ–Ω—É—î—Ç—å—Å—è")
            return
        
        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ –ø–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º
        training_params = self.get_training_parameters()
        validation_errors = self.validate_training_parameters(training_params)
        
        if validation_errors:
            error_msg = "–ü–æ–º–∏–ª–∫–∞ –≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞—Ö –Ω–∞–≤—á–∞–Ω–Ω—è:\n\n" + "\n".join(validation_errors)
            messagebox.showerror("–ü–æ–º–∏–ª–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤", error_msg)
            return
        
        self.training_stop_flag = False
        self.current_training_thread = threading.Thread(
            target=self.training_worker, args=(symbols,)
        )
        self.current_training_thread.daemon = True
        self.current_training_thread.start()

    def training_worker(self, symbols):
        """–†–æ–±–æ—á–∞ —Ñ—É–Ω–∫—Ü—ñ—è –Ω–∞–≤—á–∞–Ω–Ω—è –¥–ª—è –±–∞–≥–∞—Ç—å–æ—Ö –º–æ–¥–µ–ª–µ–π"""
        try:
            total_symbols = len(symbols)
            trained_count = 0
            failed_count = 0
            
            # –û—Ç—Ä–∏–º—É—î–º–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –∑ UI
            training_params = self.get_training_parameters()
            
            # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –≤–∞–ª—ñ–¥–Ω–æ—Å—Ç—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤
            validation_errors = self.validate_training_parameters(training_params)
            if validation_errors:
                error_msg = "‚ùå –ü–æ–º–∏–ª–∫–∞ –≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞—Ö –Ω–∞–≤—á–∞–Ω–Ω—è:\n" + "\n".join(validation_errors)
                self.update_info_text(error_msg)
                self.status_callback("–ü–æ–º–∏–ª–∫–∞ –≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞—Ö –Ω–∞–≤—á–∞–Ω–Ω—è")
                return
            
            self.update_info_text(f"üöÄ –ü–æ—á–∞—Ç–æ–∫ –Ω–∞–≤—á–∞–Ω–Ω—è {total_symbols} –º–æ–¥–µ–ª–µ–π...\n")
            
            for i, symbol in enumerate(symbols):
                if self.training_stop_flag:
                    self.update_info_text("‚èπÔ∏è –ù–∞–≤—á–∞–Ω–Ω—è –∑—É–ø–∏–Ω–µ–Ω–æ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–µ–º")
                    break
                
                # –û–Ω–æ–≤–ª–µ–Ω–Ω—è –ø—Ä–æ–≥—Ä–µ—Å—É –¥–ª—è –∑–∞–≥–∞–ª—å–Ω–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—É
                progress = (i / total_symbols) * 100
                self.update_progress(progress)
                self.status_callback(f"–ù–∞–≤—á–∞–Ω–Ω—è {symbol} ({i+1}/{total_symbols})...")
                
                try:
                    success = self.train_single_model(symbol, training_params)
                    if success:
                        trained_count += 1
                    else:
                        failed_count += 1
                        
                    # –ù–µ–≤–µ–ª–∏–∫–∞ –ø–∞—É–∑–∞ –º—ñ–∂ –Ω–∞–≤—á–∞–Ω–Ω—è–º –º–æ–¥–µ–ª–µ–π
                    time.sleep(1)
                        
                except Exception as e:
                    error_msg = f"‚ùå –ü–æ–º–∏–ª–∫–∞ –Ω–∞–≤—á–∞–Ω–Ω—è {symbol}: {str(e)}"
                    self.status_callback(error_msg)
                    self.update_info_text(error_msg)
                    failed_count += 1
                    continue
            
            # –†–µ–∑—É–ª—å—Ç–∞—Ç–∏
            result_msg = f"‚úÖ –ù–∞–≤—á–∞–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –£—Å–ø—ñ—à–Ω–æ: {trained_count}, –ù–µ –≤–¥–∞–ª–æ—Å—è: {failed_count}"
            self.status_callback(result_msg)
            self.update_info_text(f"\n{result_msg}")
            self.update_progress(100)
            
            if trained_count > 0:
                messagebox.showinfo("–†–µ–∑—É–ª—å—Ç–∞—Ç", result_msg)
                
        except Exception as e:
            error_msg = f"‚ùå –ö—Ä–∏—Ç–∏—á–Ω–∞ –ø–æ–º–∏–ª–∫–∞ –Ω–∞–≤—á–∞–Ω–Ω—è: {str(e)}"
            self.status_callback(error_msg)
            self.update_info_text(error_msg)
            self.update_progress(0)

    def train_single_model(self, symbol, training_params=None):
        """–ù–∞–≤—á–∞–Ω–Ω—è –æ–¥–Ω—ñ—î—ó –º–æ–¥–µ–ª—ñ –∑ —É—Ä–∞—Ö—É–≤–∞–Ω–Ω—è–º —É—Å—ñ—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤"""
        if training_params is None:
            training_params = self.get_training_parameters()
        
        file_path = f'data/{symbol}_data.csv'
        if not os.path.exists(file_path):
            self.update_info_text(f"‚ùå –§–∞–π–ª {file_path} –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ")
            self.add_log_message(f"‚ùå –§–∞–π–ª {file_path} –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ\n")
            return False

        try:
            self.add_log_message(f"\n{'='*60}\n")
            self.add_log_message(f"=== –ü–û–ß–ê–¢–û–ö –ù–ê–í–ß–ê–ù–ù–Ø {symbol} ===\n")
            self.add_log_message(f"{'='*60}\n\n")
            
            # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö
            self.add_log_message("üì• –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö...\n")
            data = pd.read_csv(file_path, index_col=0, parse_dates=True)
            self.add_log_message(f"‚úÖ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(data)} –∑–∞–ø–∏—Å—ñ–≤\n")
            self.add_log_message(f"üìÖ –î—ñ–∞–ø–∞–∑–æ–Ω –¥–∞–Ω–∏—Ö: {data.index[0]} –¥–æ {data.index[-1]}\n\n")

            # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –¥–∞–Ω–∏—Ö
            self.add_log_message("üîç –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —è–∫–æ—Å—Ç—ñ –¥–∞–Ω–∏—Ö...\n")
            try:
                DataValidator.check_data_requirements(data, lambda msg: self.add_log_message(f"   {msg}\n"))
                self.add_log_message("‚úÖ –î–∞–Ω—ñ –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—é—Ç—å –≤–∏–º–æ–≥–∞–º\n")
            except Exception as e:
                self.add_log_message(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ –¥–∞–Ω–∏—Ö: {str(e)}\n")
                return False

            # –ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∏ —Ç–∞ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—ñ
            crypto_type = self.classify_crypto_type(symbol, data)
            volatility_type = self.classify_asset_volatility(data, symbol)
            
            self.add_log_message(f"üîç –ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è {symbol}: –¢–∏–ø - {crypto_type}, –í–æ–ª–∞—Ç–∏–ª—å–Ω—ñ—Å—Ç—å - {volatility_type}\n")
            
            # –û—Ç—Ä–∏–º—É—î–º–æ UI –ø–∞—Ä–∞–º–µ—Ç—Ä–∏
            ui_params = {
                'epochs': self.epochs_var.get(),
                'batch_size': self.batch_size_var.get(),
                'lookback': self.lookback_var.get(),
                'learning_rate': self.lr_var.get(),
                'patience': self.patience_var.get(),
                'min_delta': self.min_delta_var.get(),
                'test_size': self.test_size_var.get(),
                'use_early_stopping': self.use_early_stopping_var.get(),
                'auto_lr': self.auto_lr_var.get()
            }
            
            # –û—Ç—Ä–∏–º—É—î–º–æ –ø—Ä–æ—Ñ—ñ–ª—å –Ω–∞–≤—á–∞–Ω–Ω—è –∑ —É—Ä–∞—Ö—É–≤–∞–Ω–Ω—è–º —Ç–∏–ø—É –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∏
            training_profile = self.get_training_profile(crypto_type, volatility_type, ui_params)
            
            # –û–Ω–æ–≤–ª—é—î–º–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –Ω–∞–≤—á–∞–Ω–Ω—è
            training_params.update(training_profile)
            
            self.add_log_message("‚öôÔ∏è –ü–ê–†–ê–ú–ï–¢–†–ò –ù–ê–í–ß–ê–ù–ù–Ø:\n")
            self.add_log_message(f"  –¢–∏–ø –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∏: {crypto_type}\n")
            self.add_log_message(f"  –¢–∏–ø –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—ñ: {volatility_type}\n")
            self.add_log_message(f"  –ï–ø–æ—Ö–∏: {training_params['epochs']}\n")
            self.add_log_message(f"  –†–æ–∑–º—ñ—Ä –±–∞—Ç—á–∞: {training_params['batch_size']}\n")
            self.add_log_message(f"  –†–æ–∑–º—ñ—Ä –≤—ñ–∫–Ω–∞: {training_params['lookback']}\n")
            self.add_log_message(f"  Learning Rate: {training_params['learning_rate']}\n")
            self.add_log_message(f"  –†–æ–∑–º—ñ—Ä —Ç–µ—Å—Ç–æ–≤–æ—ó –≤–∏–±—ñ—Ä–∫–∏: {training_params['test_size']}\n")
            self.add_log_message(f"  –ü–∞—Ç—ñ–Ω—Å: {training_params['patience']}\n")
            self.add_log_message(f"  –ú—ñ–Ω. –¥–µ–ª—å—Ç–∞: {training_params['min_delta']}\n")
            self.add_log_message(f"  –ú–∞–∫—Å. –æ–∑–Ω–∞–∫: {training_params['max_features']}\n")
            self.add_log_message(f"  –ú—ñ–Ω. –∫–æ—Ä–µ–ª—è—Ü—ñ—è: {training_params['min_correlation']}\n")
            self.add_log_message(f"  –†–∞–Ω–Ω—è –∑—É–ø–∏–Ω–∫–∞: {training_params['use_early_stopping']}\n")
            self.add_log_message(f"  –ê–≤—Ç–æ LR: {training_params['auto_lr']}\n")
            
            if training_params['use_early_stopping']:
                self.add_log_message(f"  Patience: {training_params['patience']}\n")
                self.add_log_message(f"  Min Delta: {training_params['min_delta']}\n")
            
            self.add_log_message("\n")

            # –ê–Ω–∞–ª—ñ–∑ –∫–æ—Ä–µ–ª—è—Ü—ñ—ó –æ–∑–Ω–∞–∫
            self.add_log_message("üìà –ê–ù–ê–õ–Ü–ó –ö–û–†–ï–õ–Ø–¶–Ü–á –û–ó–ù–ê–ö...\n")
            processor = DataProcessor()
            
            # –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä—ñ–≤
            df_with_indicators = processor.calculate_advanced_indicators(data)
            
            # –î–æ–¥–∞—î–º–æ —á–∞—Å–æ–≤—ñ –æ–∑–Ω–∞–∫–∏
            df_with_indicators = self.add_time_features(df_with_indicators)
            
            # –í–∏–¥–∞–ª—è—î–º–æ NaN –∑–Ω–∞—á–µ–Ω–Ω—è
            df_with_indicators = df_with_indicators.dropna()
            
            # –í–Ü–î–ë–Ü–† –û–ó–ù–ê–ö –ó–ê –¢–ò–ü–û–ú –ö–†–ò–ü–¢–û–í–ê–õ–Æ–¢–ò –¢–ê –í–û–õ–ê–¢–ò–õ–¨–ù–û–°–¢–Ü
            selected_features = self.prepare_features_by_crypto_type(df_with_indicators, crypto_type, volatility_type)
            
            # –î–æ–¥–∞—Ç–∫–æ–≤–∏–π –≤—ñ–¥–±—ñ—Ä –∑–∞ –∫–æ—Ä–µ–ª—è—Ü—ñ—î—é
            selected_features = self.filter_features_by_correlation(
                df_with_indicators, selected_features, training_params['min_correlation']
            )
            
            # –û–ë–ú–ï–ñ–ï–ù–ù–Ø –ö–Ü–õ–¨–ö–û–°–¢–Ü –û–ó–ù–ê–ö
            max_features = training_params['max_features']
            if len(selected_features) > max_features:
                selected_features = selected_features[:max_features]
            
            self.add_log_message(f"üéØ –í–Ü–î–Ü–ë–†–ê–ù–û {len(selected_features)} –û–ó–ù–ê–ö –î–õ–Ø {crypto_type} ({volatility_type}):\n")
            for i, feature in enumerate(selected_features):
                self.add_log_message(f"  {i+1:2d}. {feature}\n")
            
            if len(selected_features) == 0:
                self.add_log_message("‚ùå –ù–ï –í–î–ê–õ–û–°–Ø –í–Ü–î–Ü–ë–†–ê–¢–ò –û–ó–ù–ê–ö–ò!\n")
                return False

            # –ê–Ω–∞–ª—ñ–∑ –∫–æ—Ä–µ–ª—è—Ü—ñ—ó –¥–ª—è –ª–æ–≥—É
            correlation = self.analyze_feature_correlation(df_with_indicators[selected_features])

            # –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–∏—Ö –∑ –≤–∏–±—Ä–∞–Ω–∏–º–∏ –æ–∑–Ω–∞–∫–∞–º–∏
            self.add_log_message("üîÑ –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–∏—Ö –∑ –æ–±—Ä–∞–Ω–∏–º–∏ –æ–∑–Ω–∞–∫–∞–º–∏...\n")
            feature_data = df_with_indicators[selected_features].values
            
            scaler = StandardScaler()
            scaled_data = scaler.fit_transform(feature_data)
            
            lookback = training_params['lookback']
            X, y = [], []
            
            for i in range(lookback, len(scaled_data)):
                X.append(scaled_data[i-lookback:i])
                close_idx = selected_features.index('Close')
                y.append(scaled_data[i, close_idx])
            
            X = np.array(X)
            y = np.array(y)

            if X is None or len(X) == 0:
                self.add_log_message("‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ –¥–∞–Ω–∏—Ö –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è\n")
                return False

            self.add_log_message(f"‚úÖ –î–∞–Ω—ñ –ø—ñ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ: {X.shape[0]} samples, {X.shape[2]} features\n")
            
            # –î–µ—Ç–∞–ª—å–Ω–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –ø—Ä–æ –æ–∑–Ω–∞–∫–∏
            feature_analysis = self.analyze_features(selected_features)
            self.add_log_message("üìä –í–ò–ö–û–†–ò–°–¢–ê–ù–Ü –û–ó–ù–ê–ö–ò:\n")
            for group, features in feature_analysis.items():
                self.add_log_message(f"  {group}: {len(features)} –æ–∑–Ω–∞–∫\n")
            self.add_log_message(f"  –ó–∞–≥–∞–ª–æ–º: {len(selected_features)} –æ–∑–Ω–∞–∫\n\n")

            # –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –Ω–∞ train/test
            test_size = int(len(X) * training_params['test_size'])
            X_train, X_test = X[:-test_size], X[-test_size:]
            y_train, y_test = y[:-test_size], y[-test_size:]

            self.add_log_message("üìä –†–û–ó–ü–û–î–Ü–õ –î–ê–ù–ò–•:\n")
            self.add_log_message(f"  Train: {len(X_train)} samples ({((1-training_params['test_size'])*100):.1f}%)\n")
            self.add_log_message(f"  Test: {len(X_test)} samples ({((training_params['test_size'])*100):.1f}%)\n")
            self.add_log_message(f"  –ó–∞–≥–∞–ª–æ–º: {len(X_train) + len(X_test)} samples\n\n")

            # –ê–Ω–∞–ª—ñ–∑ –≤–∞–∂–ª–∏–≤–æ—Å—Ç—ñ –æ–∑–Ω–∞–∫
            self.add_log_message("üéØ –ê–ù–ê–õ–Ü–ó –í–ê–ñ–õ–ò–í–û–°–¢–Ü –û–ó–ù–ê–ö...\n")
            if len(selected_features) > 0:
                try:
                    feature_indices, importance_scores = self.analyze_feature_importance_correct(
                        X_train, 
                        y_train, 
                        selected_features
                    )
                except Exception as e:
                    self.add_log_message(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ –∞–Ω–∞–ª—ñ–∑—É –≤–∞–∂–ª–∏–≤–æ—Å—Ç—ñ –æ–∑–Ω–∞–∫: {str(e)}\n")
                    feature_indices, importance_scores = np.array([]), np.array([])
            else:
                self.add_log_message("‚ö†Ô∏è –ù–µ–º–∞—î –æ–∑–Ω–∞–∫ –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É –≤–∞–∂–ª–∏–≤–æ—Å—Ç—ñ\n")
                feature_indices, importance_scores = np.array([]), np.array([])

            # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ –∑–∞ —Ç–∏–ø–æ–º –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∏ —Ç–∞ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—ñ
            self.add_log_message("üß† –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ...\n")
            model = self.create_model_by_crypto_type((X_train.shape[1], X_train.shape[2]), crypto_type, volatility_type)
            self.add_log_message(f"‚úÖ –°—Ç–≤–æ—Ä–µ–Ω–æ –º–æ–¥–µ–ª—å –¥–ª—è {crypto_type} ({volatility_type})\n\n")

            # –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∏–π –ø—ñ–¥–±—ñ—Ä Learning Rate
            auto_lr = training_params.get('auto_lr', True)
            if auto_lr and len(X_train) > 100:
                self.add_log_message("üîç –ê–í–¢–û–ú–ê–¢–ò–ß–ù–ò–ô –ü–Ü–î–ë–Ü–† LEARNING RATE...\n")
                try:
                    optimal_lr = self.find_optimal_learning_rate_safe(model, X_train, y_train, X_test, y_test)
                    training_params['learning_rate'] = optimal_lr
                    
                    # –ü–µ—Ä–µ–∫–æ–º–ø—ñ–ª—è—Ü—ñ—è –º–æ–¥–µ–ª—ñ –∑ –æ–ø—Ç–∏–º–∞–ª—å–Ω–∏–º LR
                    model.compile(
                        optimizer=tf.keras.optimizers.Adam(learning_rate=optimal_lr),
                        loss='mse',
                        metrics=['mae', 'mse']
                    )
                    self.add_log_message(f"‚úÖ –í—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –æ–ø—Ç–∏–º–∞–ª—å–Ω–∏–π Learning Rate: {optimal_lr:.0e}\n\n")
                except Exception as e:
                    self.add_log_message(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ –ø—ñ–¥–±–æ—Ä—É Learning Rate: {str(e)}\n")
                    self.add_log_message("‚ÑπÔ∏è –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∏–π Learning Rate\n\n")
            else:
                self.add_log_message("‚ÑπÔ∏è –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∏–π –ø—ñ–¥–±—ñ—Ä LR –ø—Ä–æ–ø—É—â–µ–Ω–æ\n\n")

            # Callbacks
            callbacks = []
            if training_params.get('use_early_stopping', True):
                callbacks.append(EarlyStopping(
                    monitor='val_loss',
                    patience=training_params['patience'],
                    min_delta=training_params['min_delta'],
                    restore_best_weights=True,
                    verbose=1
                ))
                self.add_log_message("‚úÖ –†–∞–Ω–Ω—è –∑—É–ø–∏–Ω–∫–∞ –∞–∫—Ç–∏–≤–æ–≤–∞–Ω–∞\n")

            callbacks.append(ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.5,
                patience=max(2, training_params['patience'] // 2),
                min_lr=0.00001,
                verbose=1
            ))

            callbacks.append(TrainingProgressCallback(
                self.status_callback,
                self.update_progress,
                training_params['epochs'],
                self.add_log_message
            ))

            # –ù–∞–≤—á–∞–Ω–Ω—è
            self.add_log_message("üöÄ –ü–æ—á–∞—Ç–æ–∫ –Ω–∞–≤—á–∞–Ω–Ω—è...\n")
            self.add_log_message(f"‚è∞ –ß–∞—Å –ø–æ—á–∞—Ç–∫—É: {datetime.now().strftime('%H:%M:%S')}\n\n")
            self.status_callback(f"–ù–∞–≤—á–∞–Ω–Ω—è {symbol} ({crypto_type}, {volatility_type})...")

            history = model.fit(
                X_train, y_train,
                epochs=training_params['epochs'],
                batch_size=training_params['batch_size'],
                validation_data=(X_test, y_test),
                callbacks=callbacks,
                verbose=0
            )

            # –û—Ü—ñ–Ω–∫–∞ –º–æ–¥–µ–ª—ñ
            self.add_log_message("\nüìà –û–¶–Ü–ù–ö–ê –ú–û–î–ï–õ–Ü...\n")
            predictions = model.predict(X_test, verbose=0)
            
            # –û—Å–Ω–æ–≤–Ω—ñ –º–µ—Ç—Ä–∏–∫–∏
            mse = mean_squared_error(y_test, predictions)
            mae = mean_absolute_error(y_test, predictions)
            r2 = r2_score(y_test, predictions)
            
            # –î–µ—Ç–∞–ª—å–Ω–∏–π –∞–Ω–∞–ª—ñ–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
            detailed_metrics = self.analyze_training_results(history, X_test, y_test, model)

            # –û–Ω–æ–≤–ª–µ–Ω–Ω—è –≥—Ä–∞—Ñ—ñ–∫–∞
            self.update_training_plot(history, symbol)

            # –í–∏–∑–Ω–∞—á–µ–Ω–Ω—è –Ω–∞–π–∫—Ä–∞—â–æ—ó –µ–ø–æ—Ö–∏
            best_epoch = len(history.history['loss'])
            if 'val_loss' in history.history:
                best_epoch = np.argmin(history.history['val_loss']) + 1

            # –ê–Ω–∞–ª—ñ–∑ –ø–µ—Ä–µ–Ω–∞–≤—á–∞–Ω–Ω—è
            overfitting_info = ""
            if 'val_loss' in history.history and 'loss' in history.history:
                final_train_loss = history.history['loss'][-1]
                final_val_loss = history.history['val_loss'][-1]
                overfitting_ratio = final_val_loss / final_train_loss if final_train_loss > 0 else 1.0
                overfitting_info = f"\n  –°–ø—ñ–≤–≤—ñ–¥–Ω–æ—à–µ–Ω–Ω—è –ø–µ—Ä–µ–Ω–∞–≤—á–∞–Ω–Ω—è: {overfitting_ratio:.2f}"
                if overfitting_ratio > 1.2:
                    overfitting_info += " ‚ö†Ô∏è (–º–æ–∂–ª–∏–≤–µ –ø–µ—Ä–µ–Ω–∞–≤—á–∞–Ω–Ω—è)"
                elif overfitting_ratio < 0.8:
                    overfitting_info += " ‚ö†Ô∏è (–º–æ–∂–ª–∏–≤–µ –Ω–µ–¥–æ–Ω–∞–≤—á–∞–Ω–Ω—è)"

            # –í–∏–≤—ñ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
            self.add_log_message("üìä –†–ï–ó–£–õ–¨–¢–ê–¢–ò –ù–ê–í–ß–ê–ù–ù–Ø:\n")
            self.add_log_message(f"  MSE: {mse:.6f} (–º–µ–Ω—à–µ = –∫—Ä–∞—â–µ)\n")
            self.add_log_message(f"  MAE: {mae:.6f} (–º–µ–Ω—à–µ = –∫—Ä–∞—â–µ)\n")
            self.add_log_message(f"  R¬≤: {r2:.4f} (–±–ª–∏–∂—á–µ –¥–æ 1 = –∫—Ä–∞—â–µ)\n")
            
            if 'mape' in detailed_metrics and not np.isnan(detailed_metrics['mape']):
                self.add_log_message(f"  MAPE: {detailed_metrics['mape']:.2f}% (–º–µ–Ω—à–µ = –∫—Ä–∞—â–µ)\n")
            
            self.add_log_message(f"  –ï–ø–æ—Ö –≤–∏–∫–æ–Ω–∞–Ω–æ: {len(history.history['loss'])}/{training_params['epochs']}\n")
            self.add_log_message(f"  –ù–∞–π–∫—Ä–∞—â–∞ –µ–ø–æ—Ö–∞: {best_epoch}\n")
            
            if 'val_loss' in history.history:
                best_val_loss = np.min(history.history['val_loss'])
                self.add_log_message(f"  –ù–∞–π–∫—Ä–∞—â–∏–π val_loss: {best_val_loss:.6f}\n")
            
            self.add_log_message(overfitting_info + "\n\n")

            # –ö–æ–º–ø–ª–µ–∫—Å–Ω–∏–π –∞–Ω–∞–ª—ñ–∑ —è–∫–æ—Å—Ç—ñ
            self.add_log_message("üìä –î–ï–¢–ê–õ–¨–ù–ò–ô –ê–ù–ê–õ–Ü–ó –Ø–ö–û–°–¢–Ü...\n")

            quality_analysis = self.analyze_training_quality(
                history, X_test, y_test, model, selected_features, symbol
            )

            # –í–∏–≤—ñ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –∞–Ω–∞–ª—ñ–∑—É
            self.add_log_message(f"üèÜ –°–¢–ê–¢–£–° –ù–ê–í–ß–ê–ù–ù–Ø: {quality_analysis['status']}\n")
            self.add_log_message(f"üìà –ó–ê–ì–ê–õ–¨–ù–ò–ô –ë–ê–õ: {quality_analysis['score']}/10\n")

            # –ü–æ–ø–µ—Ä–µ–¥–∂–µ–Ω–Ω—è
            if quality_analysis['warnings']:
                self.add_log_message("‚ö†Ô∏è  –ü–û–ü–ï–†–ï–î–ñ–ï–ù–ù–Ø:\n")
                for warning in quality_analysis['warnings']:
                    self.add_log_message(f"   ‚Ä¢ {warning}\n")

            # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó
            if quality_analysis['recommendations']:
                self.add_log_message("üí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–Ü–á:\n")
                for recommendation in quality_analysis['recommendations']:
                    self.add_log_message(f"   ‚Ä¢ {recommendation}\n")

            # –î–µ—Ç–∞–ª—å–Ω—ñ –º–µ—Ç—Ä–∏–∫–∏
            self.add_log_message("üìä –î–ï–¢–ê–õ–¨–ù–Ü –ú–ï–¢–†–ò–ö–ò:\n")
            for metric, value in quality_analysis['metrics'].items():
                self.add_log_message(f"   {metric.upper()}: {value:.6f}\n")

            # –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–µ—Ç—Ä–∏–∫ –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è
            metrics = {
                'mse': float(mse),
                'mae': float(mae),
                'r2': float(r2),
                'crypto_type': crypto_type,
                'volatility_type': volatility_type,
                'feature_count': int(X_train.shape[2]),
                'samples_count': int(X_train.shape[0]),
                'best_epoch': int(best_epoch),
                'timestamp': datetime.now().isoformat(),
                'features': selected_features,
                'training_parameters': training_params,
                'quality_analysis': quality_analysis,
                'correlation_analysis': {
                    'min_correlation': training_params['min_correlation'],
                    'selected_features_count': len(selected_features),
                    'top_correlated_features': dict(correlation.head(10)) if not correlation.empty else {}
                }
            }

            # –î–æ–¥–∞—î–º–æ feature_importance
            if len(feature_indices) > 0 and len(importance_scores) > 0:
                metrics['feature_importance'] = {
                    'indices': feature_indices.tolist(),
                    'scores': importance_scores.tolist()
                }

            # –î–æ–¥–∞—î–º–æ detailed_metrics
            try:
                serializable_detailed = {}
                for key, value in detailed_metrics.items():
                    if hasattr(value, 'item'):
                        serializable_detailed[key] = value.item()
                    elif isinstance(value, (np.integer, np.int64, np.int32)):
                        serializable_detailed[key] = int(value)
                    elif isinstance(value, (np.floating, np.float64, np.float32)):
                        serializable_detailed[key] = float(value)
                    elif isinstance(value, np.ndarray):
                        serializable_detailed[key] = value.tolist()
                    else:
                        serializable_detailed[key] = value
                metrics['detailed_metrics'] = serializable_detailed
            except Exception as e:
                self.add_log_message(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—ó detailed_metrics: {e}\n")

            # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ
            self.add_log_message("üíæ –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ...\n")
            success = self.model_manager.save_model(symbol, model, None, metrics)

            if success:
                # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –ø—Ä–æ—Ñ—ñ–ª—é –∞–∫—Ç–∏–≤—É
                self.save_asset_profile(symbol, crypto_type, volatility_type, metrics)
                
                # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –∑–≤—ñ—Ç—É —è–∫–æ—Å—Ç—ñ
                self.create_quality_report(history, quality_analysis, symbol)
                
                result_message = f"""‚úÖ –£–°–ü–Ü–®–ù–û –ó–ê–í–ï–†–®–ï–ù–û: {symbol} ({crypto_type}, {volatility_type})
            MSE: {mse:.6f}, MAE: {mae:.6f}, R¬≤: {r2:.4f}
            –ï–ø–æ—Ö–∏: {len(history.history['loss'])}/{training_params['epochs']}
            –û–∑–Ω–∞–∫–∏: {len(selected_features)}
            –°—Ç–∞—Ç—É—Å: {quality_analysis['status']}
            """
                self.add_log_message(result_message + "\n")
                self.add_log_message(f"‚è∞ –ß–∞—Å –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—è: {datetime.now().strftime('%H:%M:%S')}\n")
                self.add_log_message(f"{'='*60}\n\n")
                
                self.update_info_text(result_message)
                self.status_callback(f"‚úÖ {symbol} ({crypto_type}, {volatility_type}) –Ω–∞–≤—á–µ–Ω–æ —É—Å–ø—ñ—à–Ω–æ! R¬≤: {r2:.4f}")
                
                # –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –∑–≤—ñ—Ç—É
                self.generate_training_report(symbol, history, metrics, training_params)
                
                return True
            else:
                self.add_log_message("‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ\n")
                return False

        except Exception as e:
            error_msg = f"‚ùå –ö–†–ò–¢–ò–ß–ù–ê –ü–û–ú–ò–õ–ö–ê –ù–ê–í–ß–ê–ù–ù–Ø {symbol}:\n{str(e)}\n"
            self.add_log_message(error_msg)
            self.update_info_text(error_msg)
            self.status_callback(f"–ü–æ–º–∏–ª–∫–∞ –Ω–∞–≤—á–∞–Ω–Ω—è {symbol}")
            import traceback
            traceback.print_exc()
            return False

    
    
    
    def prepare_data_advanced(self, data, params):
        """–ü–æ–∫—Ä–∞—â–µ–Ω–∞ –ø—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–∏—Ö –∑ –∞–Ω–∞–ª—ñ–∑–æ–º –∫–æ—Ä–µ–ª—è—Ü—ñ—ó"""
        processor = DataProcessor()
        
        try:
            # –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ –≤—Å—ñ—Ö —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä—ñ–≤
            df = processor.calculate_advanced_indicators(data)
            
            # –î–æ–¥–∞—î–º–æ —á–∞—Å–æ–≤—ñ –æ–∑–Ω–∞–∫–∏
            df = self.add_time_features(df)
            
            # –í–∏–¥–∞–ª—è—î–º–æ NaN –∑–Ω–∞—á–µ–Ω–Ω—è
            df = df.dropna()
            
            # –ê–Ω–∞–ª—ñ–∑ –∫–æ—Ä–µ–ª—è—Ü—ñ—ó
            correlation = self.analyze_feature_correlation(df)
            
            # –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∏–π –≤–∏–±—ñ—Ä –æ–∑–Ω–∞–∫ –Ω–∞ –æ—Å–Ω–æ–≤—ñ –∫–æ—Ä–µ–ª—è—Ü—ñ—ó
            selected_features = self.select_features_by_correlation(
                df, min_correlation=0.15, target_column='Close'
            )
            
            # –î–æ–¥–∞—î–º–æ –æ–±–æ–≤'—è–∑–∫–æ–≤—ñ –æ–∑–Ω–∞–∫–∏
            essential_features = ['Close', 'Returns', 'Volume_MA_20']
            for feature in essential_features:
                if feature in df.columns and feature not in selected_features:
                    selected_features.append(feature)
            
            # –í–∏–±—ñ—Ä–∫–∞ –¥–∞–Ω–∏—Ö
            feature_data = df[selected_features].values
            
            # –ú–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è
            scaler = StandardScaler()
            scaled_data = scaler.fit_transform(feature_data)
            
            # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç–µ–π
            lookback = params['lookback']
            X, y = [], []
            
            for i in range(lookback, len(scaled_data)):
                X.append(scaled_data[i-lookback:i])
                close_idx = selected_features.index('Close')
                y.append(scaled_data[i, close_idx])
            
            X = np.array(X)
            y = np.array(y)
            
            return X, y, selected_features
            
        except Exception as e:
            self.add_log_message(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—ñ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–∏—Ö: {str(e)}\n")
            import traceback
            traceback.print_exc()
            return None, None, None

    def add_time_features(self, df):
        """–î–æ–¥–∞–≤–∞–Ω–Ω—è —Ä–æ–∑—à–∏—Ä–µ–Ω–∏—Ö —á–∞—Å–æ–≤–∏—Ö –æ–∑–Ω–∞–∫"""
        df = df.copy()
        
        # –ë–∞–∑–æ–≤—ñ —á–∞—Å–æ–≤—ñ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∏
        df['Hour'] = df.index.hour
        df['Day_of_Week'] = df.index.dayofweek
        df['Day_of_Month'] = df.index.day
        df['Month'] = df.index.month
        df['Quarter'] = df.index.quarter
        df['Year'] = df.index.year
        
        # –¶–∏–∫–ª—ñ—á–Ω—ñ –æ–∑–Ω–∞–∫–∏
        df['Hour_sin'] = np.sin(2 * np.pi * df['Hour'] / 24)
        df['Hour_cos'] = np.cos(2 * np.pi * df['Hour'] / 24)
        df['Day_sin'] = np.sin(2 * np.pi * df['Day_of_Week'] / 7)
        df['Day_cos'] = np.cos(2 * np.pi * df['Day_of_Week'] / 7)
        df['Month_sin'] = np.sin(2 * np.pi * df['Month'] / 12)
        df['Month_cos'] = np.cos(2 * np.pi * df['Month'] / 12)
        
        # –¢–æ—Ä–≥–æ–≤—ñ —Å–µ—Å—ñ—ó
        df['Asian_Session'] = ((df['Hour'] >= 0) & (df['Hour'] <= 8)).astype(int)
        df['European_Session'] = ((df['Hour'] >= 7) & (df['Hour'] <= 16)).astype(int)
        df['US_Session'] = ((df['Hour'] >= 13) & (df['Hour'] <= 22)).astype(int)
        
        # –í–∏—Ö—ñ–¥–Ω—ñ —Ç–∞ —Å–≤—è—Ç–∫–æ–≤—ñ –¥–Ω—ñ
        df['Weekend'] = (df['Day_of_Week'] >= 5).astype(int)
        
        # –°–µ–∑–æ–Ω–Ω—ñ—Å—Ç—å
        df['Season'] = (df['Month'] % 12 + 3) // 3
        
        return df
    
    def find_optimal_learning_rate_safe(self, model, X_train, y_train, X_val, y_val):
        """–ë–µ–∑–ø–µ—á–Ω–∏–π –ø–æ—à—É–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ learning rate"""
        learning_rates = [1e-5, 3e-5, 1e-4, 3e-4, 1e-3, 3e-3, 1e-2]
        best_lr = 0.001
        best_loss = float('inf')
        
        for lr in learning_rates:
            try:
                # –°—Ç–≤–æ—Ä—é—î–º–æ –Ω–æ–≤—É –º–æ–¥–µ–ª—å –∑–∞–º—ñ—Å—Ç—å –∫–ª–æ–Ω—É–≤–∞–Ω–Ω—è
                input_shape = (X_train.shape[1], X_train.shape[2])
                temp_model = self.create_simple_advanced_model(input_shape, {'learning_rate': lr})
                
                # –ö–æ—Ä–æ—Ç–∫–µ –Ω–∞–≤—á–∞–Ω–Ω—è
                history = temp_model.fit(
                    X_train[:100], y_train[:100],  # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –ø—ñ–¥–º–Ω–æ–∂–∏–Ω—É
                    epochs=2,
                    batch_size=32,
                    validation_data=(X_val[:50], y_val[:50]),
                    verbose=0
                )
                
                val_loss = history.history['val_loss'][-1]
                self.add_log_message(f"  LR: {lr:.0e} -> Val Loss: {val_loss:.6f}\n")
                
                if val_loss < best_loss:
                    best_loss = val_loss
                    best_lr = lr
                    
            except Exception as e:
                self.add_log_message(f"  LR: {lr:.0e} -> –ü–æ–º–∏–ª–∫–∞: {str(e)}\n")
                continue
        
        return best_lr

    def create_simple_advanced_model(self, input_shape, params):
        """–°–ø—Ä–æ—â–µ–Ω–∞ –º–æ–¥–µ–ª—å –¥–ª—è –±–∞–≥–∞—Ç—å–æ—Ö –æ–∑–Ω–∞–∫"""
        model = tf.keras.Sequential()
        
        model.add(tf.keras.layers.Input(shape=input_shape))
        model.add(tf.keras.layers.BatchNormalization())
        
        model.add(tf.keras.layers.LSTM(128, return_sequences=True,
                                    kernel_regularizer=tf.keras.regularizers.l2(0.001)))
        model.add(tf.keras.layers.Dropout(0.3))
        model.add(tf.keras.layers.BatchNormalization())
        
        model.add(tf.keras.layers.LSTM(96, return_sequences=True,
                                    kernel_regularizer=tf.keras.regularizers.l2(0.001)))
        model.add(tf.keras.layers.Dropout(0.3))
        model.add(tf.keras.layers.BatchNormalization())
        
        model.add(tf.keras.layers.LSTM(64,
                                    kernel_regularizer=tf.keras.regularizers.l2(0.001)))
        model.add(tf.keras.layers.Dropout(0.4))
        model.add(tf.keras.layers.BatchNormalization())
        
        model.add(tf.keras.layers.Dense(128, activation='relu',
                                    kernel_regularizer=tf.keras.regularizers.l2(0.001)))
        model.add(tf.keras.layers.Dropout(0.3))
        model.add(tf.keras.layers.BatchNormalization())
        
        model.add(tf.keras.layers.Dense(64, activation='relu',
                                    kernel_regularizer=tf.keras.regularizers.l2(0.001)))
        model.add(tf.keras.layers.Dropout(0.3))
        
        model.add(tf.keras.layers.Dense(32, activation='relu'))
        model.add(tf.keras.layers.Dropout(0.2))
        
        model.add(tf.keras.layers.Dense(1))
        
        optimizer = tf.keras.optimizers.Adam(learning_rate=params.get('learning_rate', 0.001))
        model.compile(optimizer=optimizer, loss='mse', metrics=['mae', 'mse'])
        
        return model

    def prepare_filtered_features(self, data, min_correlation=0.15):
        """–í—ñ–¥–±—ñ—Ä —Ç—ñ–ª—å–∫–∏ —è–∫—ñ—Å–Ω–∏—Ö –æ–∑–Ω–∞–∫"""
        try:
            # –ö–æ—Ä–µ–ª—è—Ü—ñ–π–Ω–∏–π –∞–Ω–∞–ª—ñ–∑
            correlation = data.corr()['Close'].abs().sort_values(ascending=False)
            
            # –í–∏–±—ñ—Ä –æ–∑–Ω–∞–∫ –∑ –¥–æ—Å—Ç–∞—Ç–Ω—å–æ—é –∫–æ—Ä–µ–ª—è—Ü—ñ—î—é
            selected_features = correlation[correlation >= min_correlation].index.tolist()
            
            # –í–∏–¥–∞–ª—è—î–º–æ NaN –∑–Ω–∞—á–µ–Ω–Ω—è
            selected_features = [f for f in selected_features if not np.isnan(correlation[f])]
            
            # –î–æ–¥–∞—î–º–æ –æ–±–æ–≤'—è–∑–∫–æ–≤—ñ –æ–∑–Ω–∞–∫–∏
            essential = ['Close', 'Returns', 'Volume_MA_20']
            for feature in essential:
                if feature in data.columns and feature not in selected_features:
                    selected_features.append(feature)
            
            # –í–∏–¥–∞–ª—è—î–º–æ –¥—É–±–ª—ñ–∫–∞—Ç–∏
            selected_features = list(set(selected_features))
            
            return selected_features
            
        except Exception as e:
            self.add_log_message(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—ó –æ–∑–Ω–∞–∫: {str(e)}\n")
            # –ü–æ–≤–µ—Ä—Ç–∞—î–º–æ –≤—Å—ñ –æ–∑–Ω–∞–∫–∏ —É —Ä–∞–∑—ñ –ø–æ–º–∏–ª–∫–∏
            return data.columns.tolist()

    def analyze_feature_importance_correct(self, X_train, y_train, feature_names):
        """–ê–Ω–∞–ª—ñ–∑ –≤–∞–∂–ª–∏–≤–æ—Å—Ç—ñ –æ–∑–Ω–∞–∫ –±–µ–∑ –≤–∏—Ä—ñ–≤–Ω—é–≤–∞–Ω–Ω—è"""
        try:
            # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Å–µ—Ä–µ–¥–Ω—î –∑–Ω–∞—á–µ–Ω–Ω—è –ø–æ —á–∞—Å–æ–≤–∏—Ö –≤—ñ–∫–Ω–∞—Ö
            X_mean = np.mean(X_train, axis=1)  # [samples, features]
            
            from sklearn.ensemble import GradientBoostingRegressor
            gb = GradientBoostingRegressor(n_estimators=50, random_state=42, max_depth=3)
            gb.fit(X_mean, y_train)
            
            importance = gb.feature_importances_
            indices = np.argsort(importance)[::-1]
            
            self.add_log_message("üéØ –í–ê–ñ–õ–ò–í–Ü–°–¢–¨ –û–ó–ù–ê–ö (—Å–µ—Ä–µ–¥–Ω—î –ø–æ –≤—ñ–∫–Ω—É):\n")
            for i, idx in enumerate(indices):
                if importance[idx] > 0.01 and i < 15:  # –¢–æ–ø-15 –∑ –≤–∞–∂–ª–∏–≤—ñ—Å—Ç—é > 1%
                    feature_name = feature_names[idx] if idx < len(feature_names) else f'feature_{idx}'
                    self.add_log_message(f"  {i+1:2d}. {feature_name:25s}: {importance[idx]:.4f}\n")
            
            return indices, importance
            
        except Exception as e:
            self.add_log_message(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ –∞–Ω–∞–ª—ñ–∑—É –≤–∞–∂–ª–∏–≤–æ—Å—Ç—ñ: {str(e)}\n")
            return np.array([]), np.array([])
    
    def classify_asset_volatility(self, data, symbol):
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∞ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è –∞–∫—Ç–∏–≤—É –∑–∞ –≤–æ–ª–∞—Ç–∏–ª—å–Ω—ñ—Å—Ç—é"""
        returns = data['Close'].pct_change().dropna()
        
        volatility = returns.std()
        avg_daily_change = returns.abs().mean()
        max_daily_change = returns.abs().max()
        
        # –ö—Ä–∏—Ç–µ—Ä—ñ—ó –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó
        if volatility > 0.04 or max_daily_change > 0.15:
            volatility_type = 'HIGH'
        elif volatility > 0.02:
            volatility_type = 'MEDIUM' 
        else:
            volatility_type = 'LOW'
        
        self.add_log_message(f"üìä –ö–õ–ê–°–ò–§–Ü–ö–ê–¶–Ü–Ø {symbol}:\n")
        self.add_log_message(f"  –í–æ–ª–∞—Ç–∏–ª—å–Ω—ñ—Å—Ç—å: {volatility:.4f}\n")
        self.add_log_message(f"  –°–µ—Ä–µ–¥–Ω—è –∑–º—ñ–Ω–∞: {avg_daily_change:.4f}\n")
        self.add_log_message(f"  –ú–∞–∫—Å –∑–º—ñ–Ω–∞: {max_daily_change:.4f}\n")
        self.add_log_message(f"  –¢–∏–ø: {volatility_type}\n")
        
        return volatility_type

    def validate_training_parameters(self, params):
        """–ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –≤–∞–ª—ñ–¥–Ω–æ—Å—Ç—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ –Ω–∞–≤—á–∞–Ω–Ω—è"""
        errors = []
        
        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –æ—Å–Ω–æ–≤–Ω–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤
        if params['epochs'] <= 0 or params['epochs'] > 1000:
            errors.append("–ö—ñ–ª—å–∫—ñ—Å—Ç—å –µ–ø–æ—Ö –ø–æ–≤–∏–Ω–Ω–∞ –±—É—Ç–∏ –º—ñ–∂ 1 —Ç–∞ 1000")
        
        if params['batch_size'] <= 0 or params['batch_size'] > 256:
            errors.append("–†–æ–∑–º—ñ—Ä –±–∞—Ç—á–∞ –ø–æ–≤–∏–Ω–µ–Ω –±—É—Ç–∏ –º—ñ–∂ 1 —Ç–∞ 256")
        
        if params['lookback'] < 5 or params['lookback'] > 200:
            errors.append("–†–æ–∑–º—ñ—Ä –≤—ñ–∫–Ω–∞ –ø–æ–≤–∏–Ω–µ–Ω –±—É—Ç–∏ –º—ñ–∂ 5 —Ç–∞ 200")
        
        if params['learning_rate'] <= 0 or params['learning_rate'] > 0.1:
            errors.append("Learning rate –ø–æ–≤–∏–Ω–µ–Ω –±—É—Ç–∏ –º—ñ–∂ 0.0001 —Ç–∞ 0.1")
        
        if params['test_size'] <= 0 or params['test_size'] >= 1:
            errors.append("–†–æ–∑–º—ñ—Ä —Ç–µ—Å—Ç–æ–≤–æ—ó –≤–∏–±—ñ—Ä–∫–∏ –ø–æ–≤–∏–Ω–µ–Ω –±—É—Ç–∏ –º—ñ–∂ 0 —Ç–∞ 1")
        
        if params['patience'] < 1 or params['patience'] > 50:
            errors.append("–ü–∞—Ç—ñ–Ω—Å –ø–æ–≤–∏–Ω–µ–Ω –±—É—Ç–∏ –º—ñ–∂ 1 —Ç–∞ 50")
        
        if params['min_delta'] <= 0 or params['min_delta'] > 0.01:
            errors.append("–ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∞ –¥–µ–ª—å—Ç–∞ –ø–æ–≤–∏–Ω–Ω–∞ –±—É—Ç–∏ –º—ñ–∂ 0.000001 —Ç–∞ 0.01")
        
        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –¥–æ–¥–∞—Ç–∫–æ–≤–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤
        if params['dropout_start'] < 0 or params['dropout_start'] > 0.9:
            errors.append("–ü–æ—á–∞—Ç–∫–æ–≤–∏–π dropout –ø–æ–≤–∏–Ω–µ–Ω –±—É—Ç–∏ –º—ñ–∂ 0 —Ç–∞ 0.9")
        
        if params['dropout_end'] < 0 or params['dropout_end'] > 0.9:
            errors.append("–ö—ñ–Ω—Ü–µ–≤–∏–π dropout –ø–æ–≤–∏–Ω–µ–Ω –±—É—Ç–∏ –º—ñ–∂ 0 —Ç–∞ 0.9")
        
        if params['lstm_layers'] < 1 or params['lstm_layers'] > 5:
            errors.append("–ö—ñ–ª—å–∫—ñ—Å—Ç—å LSTM —à–∞—Ä—ñ–≤ –ø–æ–≤–∏–Ω–Ω–∞ –±—É—Ç–∏ –º—ñ–∂ 1 —Ç–∞ 5")
        
        if params['lstm_units_start'] < 8 or params['lstm_units_start'] > 512:
            errors.append("–ü–æ—á–∞—Ç–∫–æ–≤—ñ LSTM —é–Ω—ñ—Ç–∏ –ø–æ–≤–∏–Ω–Ω—ñ –±—É—Ç–∏ –º—ñ–∂ 8 —Ç–∞ 512")
        
        if params['lstm_units_end'] < 8 or params['lstm_units_end'] > 512:
            errors.append("–ö—ñ–Ω—Ü–µ–≤—ñ LSTM —é–Ω—ñ—Ç–∏ –ø–æ–≤–∏–Ω–Ω—ñ –±—É—Ç–∏ –º—ñ–∂ 8 —Ç–∞ 512")
        
        if params['min_correlation'] < 0 or params['min_correlation'] > 1:
            errors.append("–ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∞ –∫–æ—Ä–µ–ª—è—Ü—ñ—è –ø–æ–≤–∏–Ω–Ω–∞ –±—É—Ç–∏ –º—ñ–∂ 0 —Ç–∞ 1")
        
        return errors
    
    def get_training_profile(self, crypto_type, volatility_type, ui_params):
        """–û—Ç—Ä–∏–º–∞–Ω–Ω—è –ø—Ä–æ—Ñ—ñ–ª—é –Ω–∞–≤—á–∞–Ω–Ω—è –∑ —É—Ä–∞—Ö—É–≤–∞–Ω–Ω—è–º —Ç–∏–ø—É –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∏"""
        
        # –ë–∞–∑–æ–≤—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –∑ UI
        profile = {
            'crypto_type': crypto_type,
            'volatility_type': volatility_type,
            'lookback': ui_params['lookback'],
            'epochs': ui_params['epochs'],
            'batch_size': ui_params['batch_size'],
            'test_size': ui_params['test_size'],
            'learning_rate': ui_params['learning_rate'],
            'patience': ui_params['patience'],
            'min_delta': ui_params.get('min_delta', 0.0001),
            'use_early_stopping': ui_params.get('use_early_stopping', True),
            'auto_lr': ui_params.get('auto_lr', True),
        }
        
        # –°–ø–µ—Ü–∏—Ñ—ñ—á–Ω—ñ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ —Ç–∏–ø—É –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∏
        crypto_profiles = {
            'STABLECOIN': {
                'max_features': 8,
                'lstm_units': [64, 32],
                'dropout_rates': [0.1, 0.1],
                'required_features': ['Close', 'Returns', 'Volume_MA_20', 'MA_20'],
                'recommended_features': ['MA_50', 'Volatility_20', 'Day_of_Week'],
                'description': '–ö–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–∞ –º–æ–¥–µ–ª—å –¥–ª—è —Å—Ç–∞–±—ñ–ª—å–Ω–∏—Ö –∞–∫—Ç–∏–≤—ñ–≤'
            },
            'MEMECOIN': {
                'max_features': 15,
                'lstm_units': [128, 64, 32],
                'dropout_rates': [0.3, 0.4, 0.3],
                'required_features': ['Close', 'Returns', 'Volume', 'RSI_14', 'Volatility_20'],
                'recommended_features': ['Social_Volume', 'Twitter_Sentiment', 'MACD', 'Bollinger_Width'],
                'description': '–ê–≥—Ä–µ—Å–∏–≤–Ω–∞ –º–æ–¥–µ–ª—å –¥–ª—è –≤–∏—Å–æ–∫–æ–≤–æ–ª–∞—Ç–∏–ª—å–Ω–∏—Ö –º–µ–º–∫–æ—ñ–Ω—ñ–≤'
            },
            'ALTCOIN': {
                'max_features': 12,
                'lstm_units': [96, 64, 32],
                'dropout_rates': [0.2, 0.3, 0.2],
                'required_features': ['Close', 'Returns', 'Volume_MA_20', 'RSI_14', 'MA_20'],
                'recommended_features': ['MACD', 'BTC_Correlation', 'Market_Cap_Change', 'Volatility_20'],
                'description': '–ó–±–∞–ª–∞–Ω—Å–æ–≤–∞–Ω–∞ –º–æ–¥–µ–ª—å –¥–ª—è –∞–ª—å—Ç–∫–æ–π–Ω—ñ–≤'
            },
            'BLUECHIP': {
                'max_features': 20,
                'lstm_units': [128, 96, 64, 32],
                'dropout_rates': [0.15, 0.2, 0.15, 0.1],
                'required_features': ['Close', 'Returns', 'Volume', 'RSI_14', 'MA_20', 'MA_50'],
                'recommended_features': ['MACD', 'Bollinger_Bands', 'Volatility_50', 'Institutional_Flow'],
                'description': '–ö–æ–º–ø–ª–µ–∫—Å–Ω–∞ –º–æ–¥–µ–ª—å –¥–ª—è blue-chip –∞–∫—Ç–∏–≤—ñ–≤'
            },
            'HIGH_VOLATILITY': {
                'max_features': 10,
                'lstm_units': [64, 32],
                'dropout_rates': [0.3, 0.25],
                'required_features': ['Close', 'Returns', 'Volatility_20', 'RSI_14'],
                'recommended_features': ['ATR_14', 'Volume_Spike', 'Price_Change_5d'],
                'description': '–û–±–µ—Ä–µ–∂–Ω–∞ –º–æ–¥–µ–ª—å –¥–ª—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–∏—Ö –∞–∫—Ç–∏–≤—ñ–≤'
            }
        }
        
        # –û—Ç—Ä–∏–º—É—î–º–æ –ø—Ä–æ—Ñ—ñ–ª—å –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ç–∏–ø—É
        specific_profile = crypto_profiles.get(crypto_type, crypto_profiles['ALTCOIN'])
        profile.update(specific_profile)
        
        # –î–æ–¥–∞—î–º–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—ñ
        volatility_adjustments = {
            'LOW': {'learning_rate_multiplier': 1.1, 'patience_adjust': -2},
            'MEDIUM': {'learning_rate_multiplier': 1.0, 'patience_adjust': 0},
            'HIGH': {'learning_rate_multiplier': 0.8, 'patience_adjust': +3}
        }
        
        adjustment = volatility_adjustments.get(volatility_type, volatility_adjustments['MEDIUM'])
        profile['learning_rate'] *= adjustment['learning_rate_multiplier']
        profile['patience'] += adjustment['patience_adjust']
        
        return profile

    def prepare_features_by_volatility(self, data, volatility_type):
        """–ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –æ–∑–Ω–∞–∫ –∑–∞ —Ç–∏–ø–æ–º –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—ñ –∑ –æ–±–æ–≤'—è–∑–∫–æ–≤–∏–º–∏ –æ–∑–Ω–∞–∫–∞–º–∏"""
        
        # –û–±–æ–≤'—è–∑–∫–æ–≤—ñ –æ–∑–Ω–∞–∫–∏ –¥–ª—è –≤—Å—ñ—Ö —Ç–∏–ø—ñ–≤
        base_features = ['Close', 'Returns', 'Volume_MA_20', 'Volatility_20', 'ATR_14']
        
        if volatility_type == 'HIGH':
            additional_features = ['RSI_14', 'MACD', 'Bollinger_Width', 'MA_20', 'Price_Change_5d']
        elif volatility_type == 'MEDIUM':
            additional_features = ['RSI_14', 'MACD', 'MACD_Signal', 'Bollinger_Width', 
                                'MA_20', 'EMA_20', 'Price_Change_5d', 'OBV']
        else:  # LOW
            additional_features = ['RSI_14', 'MACD', 'MACD_Signal', 'MACD_Histogram',
                                'Bollinger_Width', 'MA_20', 'EMA_20', 'MA_50', 'EMA_50',
                                'Price_Change_5d', 'Price_Change_20d', 'OBV']
        
        # –í–∏–±—ñ—Ä —Ç—ñ–ª—å–∫–∏ –¥–æ—Å—Ç—É–ø–Ω–∏—Ö –æ–∑–Ω–∞–∫
        available_base = [f for f in base_features if f in data.columns]
        available_additional = [f for f in additional_features if f in data.columns]
        
        return available_base + available_additional
    
    def create_model_by_volatility(self, input_shape, volatility_type):
        """–°—Ç–≤–æ—Ä–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ –∑–∞ —Ç–∏–ø–æ–º –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—ñ"""
        
        if volatility_type == 'HIGH':
            # –ü—Ä–æ—Å—Ç–∞ –º–æ–¥–µ–ª—å –¥–ª—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–∏—Ö –∞–∫—Ç–∏–≤—ñ–≤
            model = tf.keras.Sequential([
                tf.keras.layers.Input(shape=input_shape),
                tf.keras.layers.LSTM(64, return_sequences=False),
                tf.keras.layers.Dropout(0.3),
                tf.keras.layers.Dense(32, activation='relu'),
                tf.keras.layers.Dense(16, activation='relu'),
                tf.keras.layers.Dense(1)
            ])
            
        elif volatility_type == 'MEDIUM':
            # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞ –º–æ–¥–µ–ª—å
            model = tf.keras.Sequential([
                tf.keras.layers.Input(shape=input_shape),
                tf.keras.layers.LSTM(96, return_sequences=True),
                tf.keras.layers.Dropout(0.3),
                tf.keras.layers.LSTM(64, return_sequences=False),
                tf.keras.layers.Dropout(0.3),
                tf.keras.layers.Dense(48, activation='relu'),
                tf.keras.layers.Dense(32, activation='relu'),
                tf.keras.layers.Dense(1)
            ])
            
        else:  # LOW
            # –°–∫–ª–∞–¥–Ω–∞ –º–æ–¥–µ–ª—å –¥–ª—è —Å—Ç–∞–±—ñ–ª—å–Ω–∏—Ö –∞–∫—Ç–∏–≤—ñ–≤
            model = tf.keras.Sequential([
                tf.keras.layers.Input(shape=input_shape),
                tf.keras.layers.LSTM(128, return_sequences=True),
                tf.keras.layers.Dropout(0.3),
                tf.keras.layers.LSTM(96, return_sequences=True),
                tf.keras.layers.Dropout(0.3),
                tf.keras.layers.LSTM(64, return_sequences=False),
                tf.keras.layers.Dropout(0.4),
                tf.keras.layers.Dense(64, activation='relu'),
                tf.keras.layers.Dropout(0.3),
                tf.keras.layers.Dense(32, activation='relu'),
                tf.keras.layers.Dense(1)
            ])
        
        return model

    def adaptive_training_parameters(self, data, symbol, base_params):
        """–ê–¥–∞–ø—Ç–∏–≤–Ω–µ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤"""
        returns = data['Close'].pct_change().dropna()
        volatility = returns.std()
        
        # –ê–¥–∞–ø—Ç–∞—Ü—ñ—è –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ –∑–∞ –≤–æ–ª–∞—Ç–∏–ª—å–Ω—ñ—Å—Ç—é
        adaptive_params = base_params.copy()
        
        # –ö–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç–∏ –∞–¥–∞–ø—Ç–∞—Ü—ñ—ó
        volatility_ratio = volatility / 0.02  # –ë–∞–∑–æ–≤–∞ –≤–æ–ª–∞—Ç–∏–ª—å–Ω—ñ—Å—Ç—å 2%
        
        # –ê–¥–∞–ø—Ç–∞—Ü—ñ—è –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤
        adaptive_params['lookback'] = int(base_params['lookback'] / volatility_ratio)
        adaptive_params['batch_size'] = int(base_params['batch_size'] / volatility_ratio)
        adaptive_params['learning_rate'] = base_params['learning_rate'] * volatility_ratio
        adaptive_params['min_correlation'] = base_params['min_correlation'] * volatility_ratio
        
        # –û–±–º–µ–∂–µ–Ω–Ω—è –∑–Ω–∞—á–µ–Ω—å
        adaptive_params['lookback'] = max(20, min(adaptive_params['lookback'], 100))
        adaptive_params['batch_size'] = max(8, min(adaptive_params['batch_size'], 64))
        adaptive_params['learning_rate'] = max(0.0001, min(adaptive_params['learning_rate'], 0.01))
        adaptive_params['min_correlation'] = max(0.1, min(adaptive_params['min_correlation'], 0.4))
        
        return adaptive_params

    
    def create_model_by_crypto_type(self, input_shape, crypto_type, volatility_type):
        """–°—Ç–≤–æ—Ä–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ –∑–∞ —Ç–∏–ø–æ–º –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∏"""
        
        model = tf.keras.Sequential()
        model.add(tf.keras.layers.Input(shape=input_shape))
        
        # –ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∏ –¥–ª—è —Ä—ñ–∑–Ω–∏—Ö —Ç–∏–ø—ñ–≤ –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç
        if crypto_type == 'STABLECOIN':
            model.add(tf.keras.layers.LSTM(64, return_sequences=True))
            model.add(tf.keras.layers.Dropout(0.1))
            model.add(tf.keras.layers.LSTM(32))
            model.add(tf.keras.layers.Dropout(0.1))
            
        elif crypto_type == 'MEMECOIN':
            model.add(tf.keras.layers.LSTM(128, return_sequences=True))
            model.add(tf.keras.layers.Dropout(0.3))
            model.add(tf.keras.layers.LSTM(64, return_sequences=True))
            model.add(tf.keras.layers.Dropout(0.4))
            model.add(tf.keras.layers.LSTM(32))
            model.add(tf.keras.layers.Dropout(0.3))
            
        elif crypto_type == 'ALTCOIN':
            model.add(tf.keras.layers.LSTM(96, return_sequences=True))
            model.add(tf.keras.layers.Dropout(0.2))
            model.add(tf.keras.layers.LSTM(64, return_sequences=True))
            model.add(tf.keras.layers.Dropout(0.3))
            model.add(tf.keras.layers.LSTM(32))
            model.add(tf.keras.layers.Dropout(0.2))
            
        else:  # BLUECHIP –∞–±–æ —ñ–Ω—à—ñ
            model.add(tf.keras.layers.LSTM(128, return_sequences=True))
            model.add(tf.keras.layers.Dropout(0.15))
            model.add(tf.keras.layers.LSTM(96, return_sequences=True))
            model.add(tf.keras.layers.Dropout(0.2))
            model.add(tf.keras.layers.LSTM(64))
            model.add(tf.keras.layers.Dropout(0.15))
            model.add(tf.keras.layers.Dense(32, activation='relu'))
        
        # –§—ñ–Ω–∞–ª—å–Ω—ñ —à–∞—Ä–∏
        model.add(tf.keras.layers.Dense(16, activation='relu'))
        model.add(tf.keras.layers.Dense(1))
        
        model.compile(
            optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
            loss='mse',
            metrics=['mae', 'mse']
        )
        
        return model

    def save_asset_profile(self, symbol, crypto_type, volatility_type, metrics):
        """–ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –ø—Ä–æ—Ñ—ñ–ª—é –∞–∫—Ç–∏–≤—É –∑ —É—Ä–∞—Ö—É–≤–∞–Ω–Ω—è–º —Ç–∏–ø—É –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∏"""
        profile_path = f'profiles/{symbol}_profile.json'
        
        profile_data = {
            'symbol': symbol,
            'crypto_type': crypto_type,
            'volatility_type': volatility_type,
            'last_trained': datetime.now().isoformat(),
            'best_r2': metrics.get('r2', 0),
            'best_mse': metrics.get('mse', 0),
            'feature_count': metrics.get('feature_count', 0),
            'recommended_features': metrics.get('features', []),
            'training_parameters': metrics.get('training_parameters', {})
        }
        
        os.makedirs('profiles', exist_ok=True)
        with open(profile_path, 'w', encoding='utf-8') as f:
            json.dump(profile_data, f, indent=2, ensure_ascii=False)
        
        return profile_data


    def filter_features_by_correlation(self, data, features, min_correlation=0.15):
        """–§—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è –æ–∑–Ω–∞–∫ –∑–∞ –º—ñ–Ω—ñ–º–∞–ª—å–Ω–æ—é –∫–æ—Ä–µ–ª—è—Ü—ñ—î—é"""
        try:
            if not features:
                return features
                
            # –û–±—á–∏—Å–ª—é—î–º–æ –∫–æ—Ä–µ–ª—è—Ü—ñ—é —Ç—ñ–ª—å–∫–∏ –¥–ª—è –æ–±—Ä–∞–Ω–∏—Ö –æ–∑–Ω–∞–∫
            correlation = data[features].corr()['Close'].abs()
            
            # –§—ñ–ª—å—Ç—Ä—É—î–º–æ –æ–∑–Ω–∞–∫–∏ –∑–∞ –∫–æ—Ä–µ–ª—è—Ü—ñ—î—é
            filtered_features = [
                feature for feature in features 
                if feature in correlation and correlation[feature] >= min_correlation
            ]
            
            # –ó–∞–≤–∂–¥–∏ –¥–æ–¥–∞—î–º–æ Close
            if 'Close' not in filtered_features and 'Close' in features:
                filtered_features.append('Close')
                
            return filtered_features
            
        except Exception as e:
            self.add_log_message(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—ó –æ–∑–Ω–∞–∫: {str(e)}\n")
            return features

    def generate_trading_recommendations(self, analysis, crypto_type, symbol):
        """–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è —Ç–æ—Ä–≥–æ–≤–∏—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ–π –Ω–∞ –æ—Å–Ω–æ–≤—ñ —Ç–∏–ø—É –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∏"""
        recommendations = []
        r2 = analysis['metrics']['r2']
        
        # –ó–∞–≥–∞–ª—å–Ω—ñ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó –Ω–∞ –æ—Å–Ω–æ–≤—ñ —è–∫–æ—Å—Ç—ñ
        if r2 > 0.8:
            recommendations.append("‚úÖ –í–∏—Å–æ–∫–∞ —è–∫—ñ—Å—Ç—å –ø—Ä–æ–≥–Ω–æ–∑—É - –º–æ–∂–Ω–∞ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ –¥–ª—è —Ç–æ—Ä–≥—ñ–≤–ª—ñ")
        elif r2 > 0.6:
            recommendations.append("‚úì –ü—Ä–∏–π–Ω—è—Ç–Ω–∞ —è–∫—ñ—Å—Ç—å –ø—Ä–æ–≥–Ω–æ–∑—É - –æ–±–µ—Ä–µ–∂–Ω–∞ —Ç–æ—Ä–≥—ñ–≤–ª—è")
        else:
            recommendations.append("‚ö†Ô∏è –ù–∏–∑—å–∫–∞ —è–∫—ñ—Å—Ç—å –ø—Ä–æ–≥–Ω–æ–∑—É - —É—Ç—Ä–∏–º–∞—Ç–∏—Å—è –≤—ñ–¥ —Ç–æ—Ä–≥—ñ–≤–ª—ñ")
        
        # –°–ø–µ—Ü–∏—Ñ—ñ—á–Ω—ñ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó –¥–ª—è —Ç–∏–ø—ñ–≤ –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç
        crypto_recommendations = {
            'STABLECOIN': [
                "‚Ä¢ –°—Ç—Ä–∞—Ç–µ–≥—ñ—è: –ê—Ä–±—ñ—Ç—Ä–∞–∂ –º—ñ–∂ –±—ñ—Ä–∂–∞–º–∏",
                "‚Ä¢ –†–∏–∑–∏–∫: –ù–∏–∑—å–∫–∏–π",
                "‚Ä¢ –ü–æ–∑–∏—Ü—ñ—è: –î–æ 20% –∫–∞–ø—ñ—Ç–∞–ª—É",
                "‚Ä¢ –¢–∞–π–º—Ñ—Ä–µ–π–º: –ö–æ—Ä–æ—Ç–∫–æ—Å—Ç—Ä–æ–∫–æ–≤—ñ —É–≥–æ–¥–∏"
            ],
            'MEMECOIN': [
                "‚Ä¢ –°—Ç—Ä–∞—Ç–µ–≥—ñ—è: –ú–æ–º–µ–Ω—Ç—É–º —Ç—Ä–µ–π–¥–∏–Ω–≥",
                "‚Ä¢ –†–∏–∑–∏–∫: –î—É–∂–µ –≤–∏—Å–æ–∫–∏–π",
                "‚Ä¢ –ü–æ–∑–∏—Ü—ñ—è: –î–æ 5% –∫–∞–ø—ñ—Ç–∞–ª—É",
                "‚Ä¢ –°—Ç–æ–ø-–ª–æ—Å—Å: 15-20%",
                "‚Ä¢ –£–≤–∞–≥–∞ –¥–æ —Å–æ—Ü—ñ–∞–ª—å–Ω–∏—Ö —Å–∏–≥–Ω–∞–ª—ñ–≤"
            ],
            'ALTCOIN': [
                "‚Ä¢ –°—Ç—Ä–∞—Ç–µ–≥—ñ—è: –¢—Ä–µ–Ω–¥–æ–≤–µ —Ç–æ—Ä–≥—ñ–≤–ª—è",
                "‚Ä¢ –†–∏–∑–∏–∫: –°–µ—Ä–µ–¥–Ω—ñ–π-–í–∏—Å–æ–∫–∏–π",
                "‚Ä¢ –ü–æ–∑–∏—Ü—ñ—è: –î–æ 10% –∫–∞–ø—ñ—Ç–∞–ª—É",
                "‚Ä¢ –°—Ç–æ–ø-–ª–æ—Å—Å: 10-15%",
                "‚Ä¢ –ú–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥ BTC –∫–æ—Ä–µ–ª—è—Ü—ñ—ó"
            ],
            'BLUECHIP': [
                "‚Ä¢ –°—Ç—Ä–∞—Ç–µ–≥—ñ—è: Swing trading",
                "‚Ä¢ –†–∏–∑–∏–∫: –°–µ—Ä–µ–¥–Ω—ñ–π",
                "‚Ä¢ –ü–æ–∑–∏—Ü—ñ—è: –î–æ 15% –∫–∞–ø—ñ—Ç–∞–ª—É",
                "‚Ä¢ –°—Ç–æ–ø-–ª–æ—Å—Å: 8-12%",
                "‚Ä¢ –§–æ–∫—É—Å –Ω–∞ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–∏—Ö —Ñ–∞–∫—Ç–æ—Ä–∞—Ö"
            ]
        }
        
        # –î–æ–¥–∞—î–º–æ —Å–ø–µ—Ü–∏—Ñ—ñ—á–Ω—ñ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó
        specific_recs = crypto_recommendations.get(crypto_type, [])
        recommendations.extend(specific_recs)
        
        # –î–æ–¥–∞—Ç–∫–æ–≤—ñ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó –Ω–∞ –æ—Å–Ω–æ–≤—ñ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—ñ
        if analysis.get('volatility_type') == 'HIGH':
            recommendations.append("‚ö° –í–∏—Å–æ–∫–∞ –≤–æ–ª–∞—Ç–∏–ª—å–Ω—ñ—Å—Ç—å - –∑–º–µ–Ω—à—ñ—Ç—å —Ä–æ–∑–º—ñ—Ä –ø–æ–∑–∏—Ü—ñ—ó")
            recommendations.append("üìä –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ ATR –¥–ª—è —Å—Ç–æ–ø-–ª–æ—Å—Å")
        
        return recommendations

    def classify_crypto_type(self, symbol, data):
        """–ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∏ –∑–∞ —Ç–∏–ø–æ–º –Ω–∞ –æ—Å–Ω–æ–≤—ñ –¥–∞–Ω–∏—Ö"""
        try:
            if len(data) < 100:
                return 'UNKNOWN'
            
            returns = data['Close'].pct_change().dropna()
            volatility = returns.std()
            avg_volume = data['Volume'].mean() if 'Volume' in data.columns else 0
            price = data['Close'].iloc[-1]
            
            # –í–∏–∑–Ω–∞—á–µ–Ω–Ω—è —Ç–∏–ø—É –Ω–∞ –æ—Å–Ω–æ–≤—ñ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
            if volatility < 0.02 and price < 10:
                return 'STABLECOIN'
            
            elif volatility > 0.06 or symbol in ['DOGE', 'SHIB', 'PEPE', 'FLOKI']:
                return 'MEMECOIN'
            
            elif volatility > 0.04 or symbol in ['LINK', 'UNI', 'AAVE', 'MATIC', 'SOL', 'DOT']:
                return 'ALTCOIN'
            
            elif symbol in ['BTC', 'ETH']:
                return 'BLUECHIP'
            
            else:
                # –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∞ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è –Ω–∞ –æ—Å–Ω–æ–≤—ñ –¥–∞–Ω–∏—Ö
                if volatility < 0.03:
                    return 'LOW_VOLATILITY'
                elif volatility > 0.05:
                    return 'HIGH_VOLATILITY'
                else:
                    return 'MEDIUM_VOLATILITY'
                    
        except Exception as e:
            self.add_log_message(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó {symbol}: {str(e)}\n")
            return 'UNKNOWN'

    def prepare_features_by_crypto_type(self, data, crypto_type, volatility_type):
        """–ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –æ–∑–Ω–∞–∫ –∑–∞ —Ç–∏–ø–æ–º –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∏"""
        
        processor = DataProcessor()
        df = processor.calculate_advanced_indicators(data)
        
        # –î–æ–¥–∞—î–º–æ —á–∞—Å–æ–≤—ñ –æ–∑–Ω–∞–∫–∏
        df = self.add_time_features(df)
        
        # –°–ø–µ—Ü–∏—Ñ—ñ—á–Ω—ñ –æ–∑–Ω–∞–∫–∏ –¥–ª—è —Ä—ñ–∑–Ω–∏—Ö —Ç–∏–ø—ñ–≤ –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç
        crypto_specific_features = {
            'STABLECOIN': [
                'Price_Stability', 'Volume_Consistency', 'MA_20_Deviation',
                'Range_5d', 'Support_Level', 'Resistance_Level'
            ],
            'MEMECOIN': [
                'Social_Volume_Index', 'Twitter_Mentions', 'Community_Growth',
                'Volume_Spike_5d', 'Price_Pump_24h', 'Trend_Strength'
            ],
            'ALTCOIN': [
                'BTC_Correlation_30d', 'Market_Cap_Rank', 'Trading_Pairs',
                'Liquidity_Score', 'Development_Activity', 'Exchange_Inflows'
            ],
            'BLUECHIP': [
                'Institutional_Flow', 'Futures_Open_Interest', 'Options_Volume',
                'Whale_Transactions', 'Network_Growth', 'Hash_Rate'
            ]
        }
        
        # –ë–∞–∑–æ–≤—ñ –æ–±–æ–≤'—è–∑–∫–æ–≤—ñ –æ–∑–Ω–∞–∫–∏
        base_features = ['Close', 'Returns', 'Volume_MA_20', 'Volatility_20']
        
        # –û—Ç—Ä–∏–º—É—î–º–æ —Å–ø–µ—Ü–∏—Ñ—ñ—á–Ω—ñ –æ–∑–Ω–∞–∫–∏ –¥–ª—è —Ç–∏–ø—É
        specific_features = crypto_specific_features.get(crypto_type, [])
        
        # –í–∏–±–∏—Ä–∞—î–º–æ —Ç—ñ–ª—å–∫–∏ –¥–æ—Å—Ç—É–ø–Ω—ñ –æ–∑–Ω–∞–∫–∏
        available_features = []
        for feature in base_features + specific_features:
            if feature in df.columns:
                available_features.append(feature)
        
        # –î–æ–¥–∞—î–º–æ —Ç–µ—Ö–Ω—ñ—á–Ω—ñ —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä–∏ –Ω–∞ –æ—Å–Ω–æ–≤—ñ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—ñ
        technical_indicators = self.select_technical_indicators(volatility_type)
        for indicator in technical_indicators:
            if indicator in df.columns and indicator not in available_features:
                available_features.append(indicator)
        
        return available_features

    def select_technical_indicators(self, volatility_type):
        """–í–∏–±—ñ—Ä —Ç–µ—Ö–Ω—ñ—á–Ω–∏—Ö —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä—ñ–≤ –Ω–∞ –æ—Å–Ω–æ–≤—ñ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—ñ"""
        if volatility_type == 'HIGH':
            return ['RSI_14', 'ATR_14', 'Bollinger_Width', 'Stochastic_K', 'Price_Change_1d']
        elif volatility_type == 'MEDIUM':
            return ['RSI_14', 'MACD', 'MA_20', 'MA_50', 'Volume_Ratio', 'Price_Change_5d']
        else:  # LOW
            return ['MA_20', 'MA_50', 'EMA_20', 'Volatility_50', 'OBV', 'Price_Change_20d']

    
    def create_advanced_model(self, training_type, input_shape, params):
        """–°—Ç–≤–æ—Ä–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ, –æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–æ—ó –¥–ª—è –±–∞–≥–∞—Ç—å–æ—Ö –æ–∑–Ω–∞–∫"""
        model = tf.keras.Sequential()
        
        # Input layer
        model.add(tf.keras.layers.Input(shape=input_shape))
        
        # Batch Normalization –¥–ª—è —Å—Ç–∞–±—ñ–ª—ñ–∑–∞—Ü—ñ—ó –Ω–∞–≤—á–∞–Ω–Ω—è
        model.add(tf.keras.layers.BatchNormalization())
        
        # –ü–µ—Ä—à–∏–π LSTM —à–∞—Ä –∑ –±—ñ–ª—å—à–æ—é –∫—ñ–ª—å–∫—ñ—Å—Ç—é —é–Ω—ñ—Ç—ñ–≤
        model.add(tf.keras.layers.LSTM(
            units=128, 
            return_sequences=True,
            kernel_regularizer=tf.keras.regularizers.l2(0.001)
        ))
        model.add(tf.keras.layers.Dropout(0.3))
        model.add(tf.keras.layers.BatchNormalization())
        
        # –î—Ä—É–≥–∏–π LSTM —à–∞—Ä
        model.add(tf.keras.layers.LSTM(
            units=96,
            return_sequences=True,
            kernel_regularizer=tf.keras.regularizers.l2(0.001)
        ))
        model.add(tf.keras.layers.Dropout(0.3))
        model.add(tf.keras.layers.BatchNormalization())
        
        # –¢—Ä–µ—Ç—ñ–π LSTM —à–∞—Ä
        model.add(tf.keras.layers.LSTM(
            units=64,
            kernel_regularizer=tf.keras.regularizers.l2(0.001)
        ))
        model.add(tf.keras.layers.Dropout(0.4))
        model.add(tf.keras.layers.BatchNormalization())
        
        # Dense —à–∞—Ä–∏ –∑ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—î—é
        model.add(tf.keras.layers.Dense(128, activation='relu',
                                    kernel_regularizer=tf.keras.regularizers.l2(0.001)))
        model.add(tf.keras.layers.Dropout(0.3))
        model.add(tf.keras.layers.BatchNormalization())
        
        model.add(tf.keras.layers.Dense(64, activation='relu',
                                    kernel_regularizer=tf.keras.regularizers.l2(0.001)))
        model.add(tf.keras.layers.Dropout(0.3))
        
        model.add(tf.keras.layers.Dense(32, activation='relu'))
        model.add(tf.keras.layers.Dropout(0.2))
        
        # Output layer
        model.add(tf.keras.layers.Dense(1))
        
        # –ö–æ–º–ø—ñ–ª—è—Ü—ñ—è –∑ –∞–¥–∞–ø—Ç–∏–≤–Ω–∏–º learning rate
        optimizer = tf.keras.optimizers.Adam(
            learning_rate=params.get('learning_rate', 0.001),
            beta_1=0.9,
            beta_2=0.999,
            epsilon=1e-07,
            amsgrad=False
        )
        
        model.compile(
            optimizer=optimizer,
            loss='huber_loss',  # –ö—Ä–∞—â–µ –¥–ª—è —Ñ—ñ–Ω–∞–Ω—Å–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö
            metrics=['mae', 'mse']
        )
        
        return model
    
    def create_model_with_attention(self, input_shape, params):
        """–ú–æ–¥–µ–ª—å –∑ –º–µ—Ö–∞–Ω—ñ–∑–º–æ–º —É–≤–∞–≥–∏ –¥–ª—è –≤–∞–∂–ª–∏–≤–∏—Ö –æ–∑–Ω–∞–∫"""
        inputs = tf.keras.layers.Input(shape=input_shape)
        
        # Batch Normalization
        x = tf.keras.layers.BatchNormalization()(inputs)
        
        # LSTM layers
        lstm_out = tf.keras.layers.LSTM(128, return_sequences=True)(x)
        lstm_out = tf.keras.layers.Dropout(0.3)(lstm_out)
        
        # Attention mechanism - –í–ò–ü–†–ê–í–õ–ï–ù–û
        attention = tf.keras.layers.Dense(1, activation='tanh')(lstm_out)
        attention = tf.keras.layers.Flatten()(attention)
        attention = tf.keras.layers.Activation('softmax')(attention)
        attention = tf.keras.layers.RepeatVector(128)(attention)
        attention = tf.keras.layers.Permute([2, 1])(attention)
        
        # Apply attention
        attended = tf.keras.layers.Multiply()([lstm_out, attention])
        
        # –í–ò–ü–†–ê–í–õ–ï–ù–ù–Ø: –ó–∞–º—ñ–Ω–∞ Lambda —à–∞—Ä—É –Ω–∞ —è–≤–Ω–∏–π
        attended = tf.keras.layers.GlobalAveragePooling1D()(attended)
        
        # Dense layers
        x = tf.keras.layers.Dense(64, activation='relu')(attended)
        x = tf.keras.layers.Dropout(0.3)(x)
        x = tf.keras.layers.BatchNormalization()(x)
        
        x = tf.keras.layers.Dense(32, activation='relu')(x)
        x = tf.keras.layers.Dropout(0.2)(x)
        
        outputs = tf.keras.layers.Dense(1)(x)
        
        model = tf.keras.Model(inputs=inputs, outputs=outputs)
        
        model.compile(
            optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),
            loss='huber_loss',
            metrics=['mae']
        )
        
        return model

    def analyze_feature_importance(self, X_train_flat, y_train, feature_names):
        """–ê–Ω–∞–ª—ñ–∑ –≤–∞–∂–ª–∏–≤–æ—Å—Ç—ñ –æ–∑–Ω–∞–∫ —á–µ—Ä–µ–∑ Gradient Boosting"""
        try:
            from sklearn.ensemble import GradientBoostingRegressor
            from sklearn.inspection import permutation_importance
            
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Ä–æ–∑–º—ñ—Ä–Ω–æ—Å—Ç—ñ
            n_features_flat = X_train_flat.shape[1]
            n_original_features = len(feature_names)
            
            self.add_log_message(f"üìä –ê–ù–ê–õ–Ü–ó –í–ê–ñ–õ–ò–í–û–°–¢–Ü –û–ó–ù–ê–ö:\n")
            self.add_log_message(f"  –†–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—å –¥–∞–Ω–∏—Ö: {X_train_flat.shape}\n")
            self.add_log_message(f"  –ö—ñ–ª—å–∫—ñ—Å—Ç—å –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∏—Ö –æ–∑–Ω–∞–∫: {n_original_features}\n")
            self.add_log_message(f"  –ö—ñ–ª—å–∫—ñ—Å—Ç—å –æ–∑–Ω–∞–∫ –ø—ñ—Å–ª—è –≤–∏—Ä—ñ–≤–Ω—é–≤–∞–Ω–Ω—è: {n_features_flat}\n")
            
            # –Ø–∫—â–æ —Ä–æ–∑–º—ñ—Ä–Ω–æ—Å—Ç—ñ –Ω–µ —Å–ø—ñ–≤–ø–∞–¥–∞—é—Ç—å, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ generic names
            if n_features_flat != n_original_features:
                self.add_log_message("‚ö†Ô∏è  –£–≤–∞–≥–∞: —Ä–æ–∑–º—ñ—Ä–Ω–æ—Å—Ç—ñ –Ω–µ —Å–ø—ñ–≤–ø–∞–¥–∞—é—Ç—å, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—Ç—å—Å—è generic names\n")
                feature_names_flat = [f'feature_{i}' for i in range(n_features_flat)]
            else:
                feature_names_flat = feature_names
            
            # –¢—Ä–µ–Ω—É—î–º–æ Gradient Boosting
            gb = GradientBoostingRegressor(n_estimators=100, random_state=42, max_depth=5)
            gb.fit(X_train_flat, y_train)
            
            # –í–∞–∂–ª–∏–≤—ñ—Å—Ç—å –æ–∑–Ω–∞–∫
            importance = gb.feature_importances_
            
            # –°–æ—Ä—Ç—É—î–º–æ –æ–∑–Ω–∞–∫–∏ –∑–∞ –≤–∞–∂–ª–∏–≤—ñ—Å—Ç—é
            indices = np.argsort(importance)[::-1]
            
            self.add_log_message("üéØ –í–ê–ñ–õ–ò–í–Ü–°–¢–¨ –û–ó–ù–ê–ö (Gradient Boosting):\n")
            for i, idx in enumerate(indices):
                if importance[idx] > 0.001:  # –ü–æ–∫–∞–∑—É—î–º–æ —Ç—ñ–ª—å–∫–∏ –≤–∞–∂–ª–∏–≤—ñ –æ–∑–Ω–∞–∫–∏
                    feature_name = feature_names_flat[idx] if idx < len(feature_names_flat) else f'feature_{idx}'
                    self.add_log_message(f"  {i+1:2d}. {feature_name:25s}: {importance[idx]:.4f}\n")
                if i >= 20:  # –û–±–º–µ–∂—É—î–º–æ –≤–∏–≤—ñ–¥
                    remaining = len(indices) - 20
                    self.add_log_message(f"  ... —ñ —â–µ {remaining} –æ–∑–Ω–∞–∫ –∑ –º–µ–Ω—à–æ—é –≤–∞–∂–ª–∏–≤—ñ—Å—Ç—é\n")
                    break
            
            # Permutation importance (–æ–±–µ—Ä–µ–∂–Ω–æ - —Ü–µ –º–æ–∂–µ –±—É—Ç–∏ –ø–æ–≤—ñ–ª—å–Ω–æ)
            try:
                if n_features_flat <= 50:  # –†–æ–±–∏–º–æ —Ç—ñ–ª—å–∫–∏ –¥–ª—è —Ä–æ–∑—É–º–Ω–æ—ó –∫—ñ–ª—å–∫–æ—Å—Ç—ñ –æ–∑–Ω–∞–∫
                    perm_importance = permutation_importance(gb, X_train_flat, y_train, 
                                                        n_repeats=3, random_state=42, n_jobs=-1)
                    
                    self.add_log_message("\nüéØ PERMUTATION IMPORTANCE (—Ç–æ–ø-10):\n")
                    sorted_idx = perm_importance.importances_mean.argsort()[::-1][:10]
                    
                    for i, idx in enumerate(sorted_idx):
                        if perm_importance.importances_mean[idx] > 0:
                            feature_name = feature_names_flat[idx] if idx < len(feature_names_flat) else f'feature_{idx}'
                            self.add_log_message(f"  {i+1:2d}. {feature_name:25s}: {perm_importance.importances_mean[idx]:.4f}\n")
                else:
                    self.add_log_message("‚ÑπÔ∏è  Permutation importance –ø—Ä–æ–ø—É—â–µ–Ω–æ (–∑–∞–±–∞–≥–∞—Ç–æ –æ–∑–Ω–∞–∫)\n")
                    
            except Exception as e:
                self.add_log_message(f"‚ö†Ô∏è  –ü–æ–º–∏–ª–∫–∞ permutation importance: {str(e)}\n")
            
            return indices, importance
            
        except Exception as e:
            self.add_log_message(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∞–Ω–∞–ª—ñ–∑—É –≤–∞–∂–ª–∏–≤–æ—Å—Ç—ñ –æ–∑–Ω–∞–∫: {str(e)}\n")
            # –ü–æ–≤–µ—Ä—Ç–∞—î–º–æ –ø—É—Å—Ç—ñ –º–∞—Å–∏–≤–∏ —É —Ä–∞–∑—ñ –ø–æ–º–∏–ª–∫–∏
            return np.array([]), np.array([])

    def prepare_multivariate_timeseries(self, data, lookback=60, forecast_horizon=5):
        """–ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –±–∞–≥–∞—Ç–æ–≤–∏–º—ñ—Ä–Ω–∏—Ö —á–∞—Å–æ–≤–∏—Ö —Ä—è–¥—ñ–≤ –¥–ª—è –≤—Å—ñ—Ö –æ–∑–Ω–∞–∫"""
        # –í—Å—ñ —á–∏—Å–ª–æ–≤—ñ –∫–æ–ª–æ–Ω–∫–∏
        numeric_columns = data.select_dtypes(include=[np.number]).columns.tolist()
        
        # –í–∏–¥–∞–ª—è—î–º–æ —Ü—ñ–ª—å–æ–≤—É –∑–º—ñ–Ω–Ω—É –∑ –æ–∑–Ω–∞–∫
        features = [col for col in numeric_columns if col != 'Close']
        
        # –ú–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è
        from sklearn.preprocessing import StandardScaler
        scaler_X = StandardScaler()
        scaler_y = StandardScaler()
        
        X_scaled = scaler_X.fit_transform(data[features])
        y_scaled = scaler_y.fit_transform(data[['Close']])
        
        # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –ø–æ—Å–ª—ñ–¥–æ–≤–Ω–æ—Å—Ç–µ–π
        X, y = [], []
        
        for i in range(lookback, len(X_scaled) - forecast_horizon):
            X.append(X_scaled[i-lookback:i])  # –í—Å—ñ –æ–∑–Ω–∞–∫–∏
            y.append(y_scaled[i+forecast_horizon-1])  # –¶—ñ–ª—å —á–µ—Ä–µ–∑ forecast_horizon –ø–µ—Ä—ñ–æ–¥—ñ–≤
        
        X = np.array(X)
        y = np.array(y)
        
        return X, y, features, scaler_X, scaler_y
    
    def train_with_all_features(self, symbol, training_params):
        """–ù–∞–≤—á–∞–Ω–Ω—è –∑ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è–º –≤—Å—ñ—Ö –æ–∑–Ω–∞–∫"""
        # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö
        data = pd.read_csv(f'data/{symbol}_data.csv', index_col=0, parse_dates=True)
        
        # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –í–°–Ü —á–∏—Å–ª–æ–≤—ñ –æ–∑–Ω–∞–∫–∏
        numeric_columns = data.select_dtypes(include=[np.number]).columns.tolist()
        self.add_log_message(f"üìä –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ {len(numeric_columns)} –æ–∑–Ω–∞–∫:\n")
        for col in numeric_columns:
            self.add_log_message(f"  ‚Ä¢ {col}\n")
        
        # –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–∏—Ö
        X, y, feature_names = self.prepare_data_advanced(data, training_params)
        
        # –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –Ω–∞ train/test
        test_size = int(len(X) * training_params['test_size'])
        X_train, X_test = X[:-test_size], X[-test_size:]
        y_train, y_test = y[:-test_size], y[-test_size:]
        
        # –ê–Ω–∞–ª—ñ–∑ –≤–∞–∂–ª–∏–≤–æ—Å—Ç—ñ –æ–∑–Ω–∞–∫
        feature_indices, importance = self.analyze_feature_importance(
            X_train.reshape(X_train.shape[0], -1), 
            y_train, 
            feature_names
        )
        
        # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ, –æ–ø—Ç–∏–º—ñ–∑–æ–≤–∞–Ω–æ—ó –¥–ª—è –±–∞–≥–∞—Ç—å–æ—Ö –æ–∑–Ω–∞–∫
        model = self.create_advanced_model('expert', (X_train.shape[1], X_train.shape[2]), training_params)
        
        # –†–∞–Ω–Ω—è –∑—É–ø–∏–Ω–∫–∞ –∑ –±—ñ–ª—å—à–∏–º patience
        callbacks = [
            EarlyStopping(
                monitor='val_loss',
                patience=15,  # –ë—ñ–ª—å—à–µ —Ç–µ—Ä–ø—ñ–Ω–Ω—è
                min_delta=0.00001,  # –ú–µ–Ω—à–∞ –º—ñ–Ω—ñ–º–∞–ª—å–Ω–∞ –∑–º—ñ–Ω–∞
                restore_best_weights=True,
                verbose=1
            ),
            ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.5,
                patience=8,
                min_lr=0.00001,
                verbose=1
            )
        ]
        
        # –ù–∞–≤—á–∞–Ω–Ω—è –∑ –±—ñ–ª—å—à–æ—é –∫—ñ–ª—å–∫—ñ—Å—Ç—é –µ–ø–æ—Ö
        history = model.fit(
            X_train, y_train,
            epochs=100,  # –ë—ñ–ª—å—à–µ –µ–ø–æ—Ö
            batch_size=32,
            validation_data=(X_test, y_test),
            callbacks=callbacks,
            verbose=0,
            shuffle=False  # –î–ª—è —á–∞—Å–æ–≤–∏—Ö —Ä—è–¥—ñ–≤
        )
        
        return history, model, X_test, y_test

    def create_ensemble_model(self, input_shape, training_params, num_models=3):
        """–°—Ç–≤–æ—Ä–µ–Ω–Ω—è –∞–Ω—Å–∞–º–±–ª—é –º–æ–¥–µ–ª–µ–π"""
        models = []
        for i in range(num_models):
            model = self.create_advanced_model('expert', input_shape, training_params)
            models.append(model)
        return models

    def train_ensemble(self, models, X_train, y_train, X_test, y_test, training_params):
        """–ù–∞–≤—á–∞–Ω–Ω—è –∞–Ω—Å–∞–º–±–ª—é"""
        histories = []
        predictions = []
        
        for i, model in enumerate(models):
            self.add_log_message(f"üèóÔ∏è –ù–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ {i+1}/{len(models)}...\n")
            
            callbacks = [
                EarlyStopping(
                    monitor='val_loss',
                    patience=training_params.get('patience', 10),
                    restore_best_weights=True
                )
            ]
            
            history = model.fit(
                X_train, y_train,
                epochs=training_params.get('epochs', 50),
                batch_size=training_params.get('batch_size', 32),
                validation_data=(X_test, y_test),
                verbose=0,
                callbacks=callbacks
            )
            
            histories.append(history)
            pred = model.predict(X_test, verbose=0)
            predictions.append(pred)
        
        # –£—Å–µ—Ä–µ–¥–Ω–µ–Ω–Ω—è –ø—Ä–æ–≥–Ω–æ–∑—ñ–≤
        ensemble_prediction = np.mean(predictions, axis=0)
        return ensemble_prediction, histories
    
    def get_training_parameters(self):
        """–û—Ç—Ä–∏–º–∞–Ω–Ω—è –≤—Å—ñ—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ –Ω–∞–≤—á–∞–Ω–Ω—è –≤–∫–ª—é—á–∞—é—á–∏ –≤–∏–±—ñ—Ä –æ–∑–Ω–∞–∫"""
        params = {
            'training_type': self.training_type_var.get(),
            'epochs': self.epochs_var.get(),
            'batch_size': self.batch_size_var.get(),
            'lookback': self.lookback_var.get(),
            'test_size': self.test_size_var.get(),
            'learning_rate': self.lr_var.get(),
            'patience': self.patience_var.get(),
            'min_delta': self.min_delta_var.get(),
            'dropout_start': self.dropout_start_var.get(),
            'dropout_end': self.dropout_end_var.get(),
            'lstm_layers': self.lstm_layers_var.get(),
            'lstm_units_start': self.lstm_units_start_var.get(),
            'lstm_units_end': self.lstm_units_end_var.get(),
            'use_technical': self.use_technical_var.get(),
            'use_time_features': self.use_time_features_var.get(),
            'use_batch_norm': self.use_batch_norm_var.get(),
            'use_l2_reg': self.use_l2_reg_var.get(),
            'use_early_stopping': self.use_early_stopping_var.get(),
            
            # –ù–æ–≤—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –¥–ª—è –≤–∏–±–æ—Ä—É –æ–∑–Ω–∞–∫
            'use_close': self.use_close_var.get(),
            'use_high_low': self.use_high_low_var.get(),
            'use_open': self.use_open_var.get(),
            'use_volume': self.use_volume_var.get(),
            'use_returns': self.use_returns_var.get(),
            'use_ma': self.use_ma_var.get(),
            'use_volatility': self.use_volatility_var.get(),
            'use_rsi': self.use_rsi_var.get(),
            'use_macd': self.use_macd_var.get(),
            'use_day_of_week': self.use_day_of_week_var.get(),
            'use_month': self.use_month_var.get(),
            'use_quarter': self.use_quarter_var.get(),
            
            # –î–æ–¥–∞—Ç–∫–æ–≤—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏
            'auto_lr': self.auto_lr_var.get(),
            'min_correlation': self.min_correlation_var.get(),
            
            # –Ü—Å–Ω—É—é—á—ñ —Ç–æ—Ä–≥–æ–≤—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏
            'use_volatility_indicators': self.use_volatility_indicators_var.get(),
            'use_momentum_indicators': self.use_momentum_indicators_var.get(),
            'use_volume_indicators': self.use_volume_indicators_var.get(),
            'use_market_indicators': self.use_market_indicators_var.get(),
            'use_risk_metrics': self.use_risk_metrics_var.get(),
            'forecast_horizon': int(self.forecast_horizon_var.get()),
            'use_price_target': self.use_price_target_var.get(),
            'use_return_target': self.use_return_target_var.get(),
            'use_signal_target': self.use_signal_target_var.get()
        }
        
        return params

    def prepare_data(self, data, training_type):
        """–ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–∏—Ö –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è"""
        processor = DataProcessor()
        
        try:
            if training_type == "basic":
                # –ë–∞–∑–æ–≤–µ –Ω–∞–≤—á–∞–Ω–Ω—è - —Ç—ñ–ª—å–∫–∏ —Ü—ñ–Ω–∏ –∑–∞–∫—Ä–∏—Ç—Ç—è
                prices = data[['Close']].values
                scaler = MinMaxScaler()
                scaled_data = scaler.fit_transform(prices)
                
                lookback = self.lookback_var.get()
                X, y = [], []
                for i in range(lookback, len(scaled_data)):
                    X.append(scaled_data[i-lookback:i, 0])
                    y.append(scaled_data[i, 0])
                
                X = np.array(X)
                y = np.array(y)
                X = np.reshape(X, (X.shape[0], X.shape[1], 1))
                
                return X, y, ['Close']
                
            elif training_type == "advanced":
                # –†–æ–∑—à–∏—Ä–µ–Ω–µ –Ω–∞–≤—á–∞–Ω–Ω—è - –æ—Å–Ω–æ–≤–Ω—ñ —Ç–µ—Ö–Ω—ñ—á–Ω—ñ —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä–∏
                df = processor.prepare_features_for_ml(data)
                features = ['Close', 'Returns', 'MA_5', 'MA_20', 'Volatility']
                
                # –í–∏–±—ñ—Ä —Ç—ñ–ª—å–∫–∏ –¥–æ—Å—Ç—É–ø–Ω–∏—Ö –æ–∑–Ω–∞–∫
                available_features = [f for f in features if f in df.columns]
                feature_data = df[available_features].values
                
                scaler = MinMaxScaler()
                scaled_data = scaler.fit_transform(feature_data)
                
                lookback = self.lookback_var.get()
                X, y = [], []
                for i in range(lookback, len(scaled_data)):
                    X.append(scaled_data[i-lookback:i])
                    y.append(scaled_data[i, 0])  # –¶—ñ–ª—å - Close price
                
                X = np.array(X)
                y = np.array(y)
                
                return X, y, available_features
                
            else:  # expert
                # –ï–∫—Å–ø–µ—Ä—Ç–Ω–µ –Ω–∞–≤—á–∞–Ω–Ω—è - —Å–ø—Ä–æ—â–µ–Ω–∞ –≤–µ—Ä—Å—ñ—è
                df = data.copy()
                
                # –î–æ–¥–∞—î–º–æ –±–∞–∑–æ–≤—ñ —Ç–µ—Ö–Ω—ñ—á–Ω—ñ —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä–∏
                if 'Close' in df.columns:
                    df['Returns'] = df['Close'].pct_change()
                    df['MA_5'] = df['Close'].rolling(window=5).mean()
                    df['MA_20'] = df['Close'].rolling(window=20).mean()
                    df['Volatility'] = df['Close'].rolling(window=20).std()
                
                # –í–∏–¥–∞–ª—è—î–º–æ NaN
                df = df.dropna()
                
                # –í–∏–±–∏—Ä–∞—î–º–æ –≤—Å—ñ —á–∏—Å–ª–æ–≤—ñ –∫–æ–ª–æ–Ω–∫–∏
                numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()
                feature_data = df[numeric_columns].values
                
                scaler = StandardScaler()
                scaled_data = scaler.fit_transform(feature_data)
                
                lookback = self.lookback_var.get()
                X, y = [], []
                for i in range(lookback, len(scaled_data)):
                    X.append(scaled_data[i-lookback:i])
                    y.append(scaled_data[i, numeric_columns.index('Close')])  # –¶—ñ–ª—å - Close price
                
                X = np.array(X)
                y = np.array(y)
                
                return X, y, numeric_columns
                
        except Exception as e:
            self.info_text.insert(tk.END, f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—ñ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–∏—Ö: {str(e)}\n")
            return None, None, None
    
    def create_model(self, training_type, input_shape):
        """–°—Ç–≤–æ—Ä–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ –Ω–µ–π—Ä–æ–º–µ—Ä–µ–∂—ñ (–¥–ª—è –∑–≤–æ—Ä–æ—Ç–Ω–æ—ó —Å—É–º—ñ—Å–Ω–æ—Å—Ç—ñ)"""
        model = tf.keras.Sequential()
        
        # –î–æ–¥–∞—î–º–æ Input —à–∞—Ä —è–≤–Ω–æ
        model.add(tf.keras.layers.Input(shape=input_shape))
        
        if training_type == "basic":
            model.add(tf.keras.layers.LSTM(64, return_sequences=True))
            model.add(tf.keras.layers.Dropout(0.2))
            model.add(tf.keras.layers.LSTM(32))
            model.add(tf.keras.layers.Dropout(0.2))
            model.add(tf.keras.layers.Dense(16, activation='relu'))
            model.add(tf.keras.layers.Dense(1))
            
        elif training_type == "advanced":
            model.add(tf.keras.layers.LSTM(100, return_sequences=True))
            model.add(tf.keras.layers.Dropout(0.3))
            model.add(tf.keras.layers.LSTM(75, return_sequences=True))
            model.add(tf.keras.layers.Dropout(0.3))
            model.add(tf.keras.layers.LSTM(50))
            model.add(tf.keras.layers.Dropout(0.3))
            model.add(tf.keras.layers.Dense(25, activation='relu'))
            model.add(tf.keras.layers.Dense(1))
            
        else:  # expert
            model.add(tf.keras.layers.LSTM(128, return_sequences=True))
            model.add(tf.keras.layers.Dropout(0.4))
            model.add(tf.keras.layers.LSTM(96, return_sequences=True))
            model.add(tf.keras.layers.Dropout(0.4))
            model.add(tf.keras.layers.LSTM(64))
            model.add(tf.keras.layers.Dropout(0.4))
            model.add(tf.keras.layers.Dense(48, activation='relu'))
            model.add(tf.keras.layers.Dropout(0.3))
            model.add(tf.keras.layers.Dense(32, activation='relu'))
            model.add(tf.keras.layers.Dense(1))
        
        model.compile(
            optimizer=tf.keras.optimizers.Adam(learning_rate=self.lr_var.get()),
            loss='mse',
            metrics=['mae', 'mse']
        )
        
        return model

    def update_training_plot(self, history, symbol):
        """–û–Ω–æ–≤–ª–µ–Ω–Ω—è –≥—Ä–∞—Ñ—ñ–∫–∞ –Ω–∞–≤—á–∞–Ω–Ω—è"""
        self.ax.clear()
        
        if not history or not history.history:
            return
        
        # –ì—Ä–∞—Ñ—ñ–∫ –≤—Ç—Ä–∞—Ç
        epochs = range(1, len(history.history['loss']) + 1)
        self.ax.plot(epochs, history.history['loss'], label='Training Loss', linewidth=2, color='blue')
        
        if 'val_loss' in history.history:
            self.ax.plot(epochs, history.history['val_loss'], label='Validation Loss', linewidth=2, color='red')
        
        self.ax.set_title(f'–ù–∞–≤—á–∞–Ω–Ω—è {symbol}', fontsize=14, fontweight='bold')
        self.ax.set_xlabel('–ï–ø–æ—Ö–∞')
        self.ax.set_ylabel('Loss')
        self.ax.legend()
        self.ax.grid(True, alpha=0.3)
        
        self.canvas.draw()
    
    def update_progress(self, value):
        """–û–Ω–æ–≤–ª–µ–Ω–Ω—è –ø—Ä–æ–≥—Ä–µ—Å-–±–∞—Ä—É"""
        self.progress_bar['value'] = value
        self.parent.update_idletasks()
    
    def stop_training(self):
        """–ó—É–ø–∏–Ω–∫–∞ –Ω–∞–≤—á–∞–Ω–Ω—è"""
        self.training_stop_flag = True
        self.status_label.config(text="–ù–∞–≤—á–∞–Ω–Ω—è –∑—É–ø–∏–Ω—è—î—Ç—å—Å—è...", foreground="orange")
    
    def cleanup_models(self):
        """–û—á–∏—â–µ–Ω–Ω—è –º–æ–¥–µ–ª–µ–π"""
        if messagebox.askyesno("–ü—ñ–¥—Ç–≤–µ—Ä–¥–∂–µ–Ω–Ω—è", "–í–∏–¥–∞–ª–∏—Ç–∏ –≤—Å—ñ –Ω–∞–≤—á–µ–Ω—ñ –º–æ–¥–µ–ª—ñ?"):
            try:
                models_dir = 'models'
                if os.path.exists(models_dir):
                    for file in os.listdir(models_dir):
                        file_path = os.path.join(models_dir, file)
                        if os.path.isfile(file_path):
                            os.remove(file_path)
                    self.status_callback("–í—Å—ñ –º–æ–¥–µ–ª—ñ –≤–∏–¥–∞–ª–µ–Ω—ñ")
                    self.info_text.insert(tk.END, "‚úÖ –í—Å—ñ –Ω–∞–≤—á–µ–Ω—ñ –º–æ–¥–µ–ª—ñ –≤–∏–¥–∞–ª–µ–Ω—ñ\n")
                else:
                    self.status_callback("–ü–∞–ø–∫–∞ models –Ω–µ —ñ—Å–Ω—É—î")
                    self.info_text.insert(tk.END, "‚ÑπÔ∏è –ü–∞–ø–∫–∞ models –Ω–µ —ñ—Å–Ω—É—î\n")
            except Exception as e:
                error_msg = f"‚ùå –ü–æ–º–∏–ª–∫–∞ –æ—á–∏—â–µ–Ω–Ω—è: {str(e)}"
                self.status_callback(error_msg)
                self.info_text.insert(tk.END, f"{error_msg}\n")


    def auto_select_features(self, scenario="trading"):
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∏–π –≤–∏–±—ñ—Ä –æ–∑–Ω–∞–∫ –¥–ª—è —Ä—ñ–∑–Ω–∏—Ö —Å—Ü–µ–Ω–∞—Ä—ñ—ó–≤"""
        if scenario == "minimal":
            self.use_close_var.set(True)
            self.use_returns_var.set(True)
            self.use_ma_var.set(False)
            self.use_volatility_var.set(False)
            self.use_rsi_var.set(False)
            self.use_macd_var.set(False)
            self.use_day_of_week_var.set(True)
            self.use_month_var.set(False)
            
        elif scenario == "trading":
            self.use_close_var.set(True)
            self.use_high_low_var.set(True)
            self.use_volume_var.set(True)
            self.use_returns_var.set(True)
            self.use_ma_var.set(True)
            self.use_volatility_var.set(True)
            self.use_rsi_var.set(True)
            self.use_macd_var.set(False)
            self.use_day_of_week_var.set(True)
            self.use_month_var.set(True)
            
        elif scenario == "comprehensive":
            self.use_close_var.set(True)
            self.use_high_low_var.set(True)
            self.use_open_var.set(True)
            self.use_volume_var.set(True)
            self.use_returns_var.set(True)
            self.use_ma_var.set(True)
            self.use_volatility_var.set(True)
            self.use_rsi_var.set(True)
            self.use_macd_var.set(True)
            self.use_day_of_week_var.set(True)
            self.use_month_var.set(True)
            self.use_quarter_var.set(True)

    def analyze_training_quality(self, history, X_test, y_test, model, selected_features, symbol):
        """–ö–æ–º–ø–ª–µ–∫—Å–Ω–∏–π –∞–Ω–∞–ª—ñ–∑ —è–∫–æ—Å—Ç—ñ –Ω–∞–≤—á–∞–Ω–Ω—è –∑ –ø—Ä–∞–≤–∏–ª—å–Ω–∏–º–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—è–º–∏"""
        analysis = {
            'score': 0,
            'status': 'UNKNOWN',
            'issues': [],
            'recommendations': [],
            'metrics': {},
            'warnings': []
        }
        
        try:
            # –û—Ç—Ä–∏–º—É—î–º–æ –ø—Ä–æ–≥–Ω–æ–∑–∏
            predictions = model.predict(X_test, verbose=0)
            
            # –û—Å–Ω–æ–≤–Ω—ñ –º–µ—Ç—Ä–∏–∫–∏
            analysis['metrics']['mse'] = float(mean_squared_error(y_test, predictions))
            analysis['metrics']['mae'] = float(mean_absolute_error(y_test, predictions))
            analysis['metrics']['r2'] = float(r2_score(y_test, predictions))
            
            # –î–æ–¥–∞—Ç–∫–æ–≤—ñ –º–µ—Ç—Ä–∏–∫–∏
            analysis['metrics']['std_error'] = float(np.std(y_test - predictions.flatten()))
            analysis['metrics']['max_error'] = float(np.max(np.abs(y_test - predictions.flatten())))
            
            # –ê–Ω–∞–ª—ñ–∑ R¬≤ - –û–°–ù–û–í–ù–û–ô –∫—Ä–∏—Ç–µ—Ä—ñ–π —è–∫–æ—Å—Ç—ñ
            if analysis['metrics']['r2'] > 0.9:
                analysis['score'] += 4
                r2_status = "–í—ñ–¥–º—ñ–Ω–Ω–æ"
            elif analysis['metrics']['r2'] > 0.8:
                analysis['score'] += 3
                r2_status = "–î—É–∂–µ –¥–æ–±—Ä–µ"
            elif analysis['metrics']['r2'] > 0.7:
                analysis['score'] += 2
                r2_status = "–î–æ–±—Ä–µ"
            elif analysis['metrics']['r2'] > 0.5:
                analysis['score'] += 1
                r2_status = "–ó–∞–¥–æ–≤—ñ–ª—å–Ω–æ"
            elif analysis['metrics']['r2'] > 0.3:
                analysis['score'] += 0
                r2_status = "–°–ª–∞–±–∫–æ"
            elif analysis['metrics']['r2'] > 0:
                analysis['score'] -= 1
                r2_status = "–ü–æ–≥–∞–Ω–æ"
                analysis['issues'].append('r2_low')
                analysis['warnings'].append(f'R¬≤ –∑–∞–Ω–∞–¥—Ç–æ –Ω–∏–∑—å–∫–∏–π ({analysis["metrics"]["r2"]:.4f})')
            else:
                analysis['score'] -= 3
                r2_status = "–î—É–∂–µ –ø–æ–≥–∞–Ω–æ"
                analysis['issues'].append('r2_negative')
                analysis['warnings'].append(f'R¬≤ –Ω–µ–≥–∞—Ç–∏–≤–Ω–∏–π ({analysis["metrics"]["r2"]:.4f})')
            
            # –ê–Ω–∞–ª—ñ–∑ —ñ—Å—Ç–æ—Ä—ñ—ó –Ω–∞–≤—á–∞–Ω–Ω—è
            if history and len(history.history) > 5:
                train_loss = history.history['loss']
                val_loss = history.history.get('val_loss', [])
                
                if val_loss:
                    # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –ø–µ—Ä–µ–Ω–∞–≤—á–∞–Ω–Ω—è/–Ω–µ–¥–æ–Ω–∞–≤—á–∞–Ω–Ω—è
                    final_ratio = val_loss[-1] / train_loss[-1] if train_loss[-1] > 0 else 1
                    
                    if final_ratio > 2.0:
                        analysis['issues'].append('overfitting')
                        analysis['warnings'].append(f'–ú–æ–∂–ª–∏–≤–µ –ø–µ—Ä–µ–Ω–∞–≤—á–∞–Ω–Ω—è (—Å–ø—ñ–≤–≤—ñ–¥–Ω–æ—à–µ–Ω–Ω—è: {final_ratio:.2f})')
                        analysis['score'] -= 2
                    elif final_ratio > 1.5:
                        analysis['warnings'].append(f'–ù–µ–∑–Ω–∞—á–Ω–µ –ø–µ—Ä–µ–Ω–∞–≤—á–∞–Ω–Ω—è (—Å–ø—ñ–≤–≤—ñ–¥–Ω–æ—à–µ–Ω–Ω—è: {final_ratio:.2f})')
                        analysis['score'] -= 1
                    elif final_ratio < 0.7:
                        analysis['issues'].append('underfitting')
                        analysis['warnings'].append(f'–ú–æ–∂–ª–∏–≤–µ –Ω–µ–¥–æ–Ω–∞–≤—á–∞–Ω–Ω—è (—Å–ø—ñ–≤–≤—ñ–¥–Ω–æ—à–µ–Ω–Ω—è: {final_ratio:.2f})')
                        analysis['score'] -= 1
                    elif final_ratio < 0.9:
                        analysis['warnings'].append(f'–ù–µ–∑–Ω–∞—á–Ω–µ –Ω–µ–¥–æ–Ω–∞–≤—á–∞–Ω–Ω—è (—Å–ø—ñ–≤–≤—ñ–¥–Ω–æ—à–µ–Ω–Ω—è: {final_ratio:.2f})')
                        analysis['score'] -= 0.5
                    
                    # –í–∏–∑–Ω–∞—á–µ–Ω–Ω—è –Ω–∞–π–∫—Ä–∞—â–æ—ó –µ–ø–æ—Ö–∏
                    best_epoch = np.argmin(val_loss) + 1
                    epochs_completed = len(train_loss)
                    
                    if best_epoch < epochs_completed * 0.8:
                        analysis['warnings'].append(f'–†–∞–Ω–Ω—è –∑—É–ø–∏–Ω–∫–∞ –Ω–∞ –µ–ø–æ—Å—ñ {best_epoch}/{epochs_completed}')
            
            # –ê–Ω–∞–ª—ñ–∑ –æ–∑–Ω–∞–∫
            if len(selected_features) < 5:
                analysis['issues'].append('few_features')
                analysis['warnings'].append(f'–ó–∞–º–∞–ª–æ –æ–∑–Ω–∞–∫: {len(selected_features)}')
                analysis['score'] -= 1
            elif len(selected_features) > 25:
                analysis['issues'].append('many_features')
                analysis['warnings'].append(f'–ó–∞–±–∞–≥–∞—Ç–æ –æ–∑–Ω–∞–∫: {len(selected_features)}')
                analysis['score'] -= 1
            else:
                analysis['score'] += 1
            
            # –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –ö–û–†–ï–ö–¢–ù–ò–• —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ–π
            analysis['recommendations'] = self.generate_correct_recommendations(
                analysis, history, len(selected_features), symbol
            )
            
            # –§—ñ–Ω–∞–ª—å–Ω–∞ –æ—Ü—ñ–Ω–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤—ñ R¬≤ —è–∫ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∫—Ä–∏—Ç–µ—Ä—ñ—é
            analysis = self.calculate_final_score_based_on_r2(analysis)
            
        except Exception as e:
            analysis['warnings'].append(f'–ü–æ–º–∏–ª–∫–∞ –∞–Ω–∞–ª—ñ–∑—É: {str(e)}')
            analysis['status'] = 'ERROR'
        
        return analysis

    def calculate_final_score_based_on_r2(self, analysis):
        """–†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ —Ñ—ñ–Ω–∞–ª—å–Ω–æ—ó –æ—Ü—ñ–Ω–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤—ñ R¬≤ —è–∫ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∫—Ä–∏—Ç–µ—Ä—ñ—é"""
        r2 = analysis['metrics']['r2']
        
        # R¬≤ - –æ—Å–Ω–æ–≤–Ω–∏–π –∫—Ä–∏—Ç–µ—Ä—ñ–π —è–∫–æ—Å—Ç—ñ
        if r2 > 0.9:
            analysis['status'] = 'EXCELLENT'
            analysis['score'] = 9 + min(analysis.get('score', 0), 1)  # 9-10 –±–∞–ª—ñ–≤
        elif r2 > 0.8:
            analysis['status'] = 'VERY_GOOD'
            analysis['score'] = 8 + min(analysis.get('score', 0), 2)  # 8-10 –±–∞–ª—ñ–≤
        elif r2 > 0.7:
            analysis['status'] = 'GOOD'
            analysis['score'] = 7 + min(analysis.get('score', 0), 3)  # 7-10 –±–∞–ª—ñ–≤
        elif r2 > 0.6:
            analysis['status'] = 'FAIR'
            analysis['score'] = 6 + min(analysis.get('score', 0), 4)  # 6-10 –±–∞–ª—ñ–≤
        elif r2 > 0.4:
            analysis['status'] = 'POOR'
            analysis['score'] = 4 + min(analysis.get('score', 0), 6)  # 4-10 –±–∞–ª—ñ–≤
        else:
            analysis['status'] = 'FAILED'
            analysis['score'] = max(0, min(analysis.get('score', 0), 3))  # 0-3 –±–∞–ª–∏
        
        # –û–±–º–µ–∂—É—î–º–æ score –≤—ñ–¥ 0 –¥–æ 10
        analysis['score'] = min(max(analysis['score'], 0), 10)
        
        return analysis

    def generate_correct_recommendations(self, analysis, history, feature_count, symbol):
        """–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –∫–æ—Ä–µ–∫—Ç–Ω–∏—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ–π –Ω–∞ –æ—Å–Ω–æ–≤—ñ –∞–Ω–∞–ª—ñ–∑—É"""
        recommendations = []
        r2 = analysis['metrics']['r2']
        
        # –ó–∞–≥–∞–ª—å–Ω—ñ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó –Ω–∞ –æ—Å–Ω–æ–≤—ñ R¬≤
        if r2 > 0.8:
            recommendations.append("‚úÖ –í—ñ–¥–º—ñ–Ω–Ω–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è!")
            if r2 > 0.9:
                recommendations.append("‚≠ê –ú–æ–¥–µ–ª—å –º–∞—î –¥—É–∂–µ –≤–∏—Å–æ–∫—É —Ç–æ—á–Ω—ñ—Å—Ç—å")
        elif r2 > 0.6:
            recommendations.append("‚úì –î–æ–±—Ä–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è")
        else:
            recommendations.append("‚ö†Ô∏è –†–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ—Ç—Ä–µ–±—É—î –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è")
        
        # –°–ø–µ—Ü–∏—Ñ—ñ—á–Ω—ñ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó –Ω–∞ –æ—Å–Ω–æ–≤—ñ –ø—Ä–æ–±–ª–µ–º
        if 'few_features' in analysis['issues']:
            recommendations.append("‚Ä¢ –î–æ–¥–∞–π—Ç–µ –±—ñ–ª—å—à–µ —Ç–µ—Ö–Ω—ñ—á–Ω–∏—Ö —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä—ñ–≤ (MA, RSI, MACD, Bollinger Bands)")
            recommendations.append("‚Ä¢ –ü–µ—Ä–µ–≤—ñ—Ä—Ç–µ –Ω–∞—è–≤–Ω—ñ—Å—Ç—å —á–∞—Å–æ–≤–∏—Ö –æ–∑–Ω–∞–∫")
        
        if 'overfitting' in analysis['issues']:
            recommendations.append("‚Ä¢ –ó–º–µ–Ω—à—ñ—Ç—å –∫—ñ–ª—å–∫—ñ—Å—Ç—å —à–∞—Ä—ñ–≤ LSTM")
            recommendations.append("‚Ä¢ –ó–±—ñ–ª—å—à—Ç–µ dropout rate")
            recommendations.append("‚Ä¢ –î–æ–¥–∞–π—Ç–µ L2 regularization")
        
        if 'underfitting' in analysis['issues']:
            recommendations.append("‚Ä¢ –ó–±—ñ–ª—å—à—ñ—Ç—å –∫—ñ–ª—å–∫—ñ—Å—Ç—å –µ–ø–æ—Ö –Ω–∞–≤—á–∞–Ω–Ω—è")
            recommendations.append("‚Ä¢ –ó–±—ñ–ª—å—à—ñ—Ç—å –∫—ñ–ª—å–∫—ñ—Å—Ç—å –Ω–µ–π—Ä–æ–Ω—ñ–≤ —É —à–∞—Ä–∞—Ö")
            recommendations.append("‚Ä¢ –ó–º–µ–Ω—à—ñ—Ç—å learning rate")
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó —â–æ–¥–æ –∑—É–ø–∏–Ω–∫–∏ –Ω–∞–≤—á–∞–Ω–Ω—è
        if history and 'val_loss' in history.history:
            val_loss = history.history['val_loss']
            best_epoch = np.argmin(val_loss) + 1
            total_epochs = len(val_loss)
            
            if best_epoch < total_epochs * 0.7:
                recommendations.append(f"‚Ä¢ –ú–æ–¥–µ–ª—å –¥–æ—Å—è–≥–ª–∞ –æ–ø—Ç–∏–º—É–º—É –Ω–∞ –µ–ø–æ—Å—ñ {best_epoch} - –º–æ–∂–Ω–∞ –∑–º–µ–Ω—à–∏—Ç–∏ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –µ–ø–æ—Ö")
            elif best_epoch == total_epochs:
                recommendations.append("‚Ä¢ –ú–æ–¥–µ–ª—å –ø—Ä–æ–¥–æ–≤–∂—É—î –≤—á–∏—Ç–∏—Å—è - —Å–ø—Ä–æ–±—É–π—Ç–µ –∑–±—ñ–ª—å—à–∏—Ç–∏ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –µ–ø–æ—Ö")
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∏—Ö –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç
        if symbol in ['LINK', 'XRP', 'ADA', 'DOT']:  # –ê–ª—å—Ç–∫–æ–π–Ω–∏
            recommendations.append("‚Ä¢ –î–ª—è –∞–ª—å—Ç–∫–æ–π–Ω—ñ–≤ –¥–æ–¥–∞–π—Ç–µ –∫–æ—Ä–µ–ª—è—Ü—ñ—é –∑ BTC")
            recommendations.append("‚Ä¢ –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ –æ–±'—î–º–Ω—ñ —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä–∏")
        
        elif symbol in ['BTC', 'ETH']:  # –û—Å–Ω–æ–≤–Ω—ñ –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∏
            recommendations.append("‚Ä¢ –î–ª—è BTC/ETH –º–æ–∂–Ω–∞ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ –±—ñ–ª—å—à —Å–∫–ª–∞–¥–Ω—ñ –º–æ–¥–µ–ª—ñ")
            recommendations.append("‚Ä¢ –î–æ–¥–∞–π—Ç–µ —Ä–∏–Ω–∫–æ–≤—ñ —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä–∏ (Fear & Greed Index)")
        
        # –£–Ω—ñ–∫–∞–ª—å–Ω—ñ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó –Ω–∞ –æ—Å–Ω–æ–≤—ñ –∫—ñ–ª—å–∫–æ—Å—Ç—ñ –æ–∑–Ω–∞–∫
        if feature_count < 8:
            recommendations.append("‚Ä¢ –ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –æ–∑–Ω–∞–∫: 8-12")
        
        return recommendations

    
    def calculate_final_score(self, analysis):
        """–†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ —Ñ—ñ–Ω–∞–ª—å–Ω–æ—ó –æ—Ü—ñ–Ω–∫–∏ —è–∫–æ—Å—Ç—ñ"""
        score = analysis['score']
        
        # –î–æ–¥–∞—Ç–∫–æ–≤—ñ –±–∞–ª–∏ –∑–∞ —Å—Ç–∞–±—ñ–ª—å–Ω—ñ—Å—Ç—å
        if not analysis['issues']:
            score += 2
        
        # –í–∏–∑–Ω–∞—á–µ–Ω–Ω—è —Å—Ç–∞—Ç—É—Å—É
        if score >= 7:
            analysis['status'] = 'EXCELLENT'
        elif score >= 5:
            analysis['status'] = 'GOOD'
        elif score >= 3:
            analysis['status'] = 'FAIR'
        elif score >= 0:
            analysis['status'] = 'POOR'
        else:
            analysis['status'] = 'FAILED'
        
        analysis['score'] = min(max(score, 0), 10)  # –û–±–º–µ–∂—É—î–º–æ 0-10
        return analysis
    
    def generate_recommendations(self, analysis, history, feature_count):
        """–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∏—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ–π"""
        recommendations = []
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó –Ω–∞ –æ—Å–Ω–æ–≤—ñ –ø—Ä–æ–±–ª–µ–º
        if 'r2_negative' in analysis['issues']:
            recommendations.extend([
                "üö® –°–ü–û–ß–ê–¢–ö–£ –í–ò–†–Ü–®–ò–¢–¨ –ü–†–û–ë–õ–ï–ú–ò!",
                "‚Ä¢ –ü–µ—Ä–µ–≤—ñ—Ä—Ç–µ —è–∫—ñ—Å—Ç—å –¥–∞–Ω–∏—Ö –Ω–∞ –Ω–∞—è–≤–Ω—ñ—Å—Ç—å NaN",
                "‚Ä¢ –°–ø—Ä–æ—Å—Ç—ñ—Ç—å –º–æ–¥–µ–ª—å –¥–æ –±–∞–∑–æ–≤–æ–≥–æ —Ä—ñ–≤–Ω—è",
                "‚Ä¢ –ü–µ—Ä–µ–≤—ñ—Ä—Ç–µ —Ü—ñ–ª—å–æ–≤—É –∑–º—ñ–Ω–Ω—É –Ω–∞ –≤–∏–∫–∏–¥–∏"
            ])
        
        elif 'r2_low' in analysis['issues']:
            recommendations.extend([
                "‚Ä¢ –ó–±—ñ–ª—å—à—ñ—Ç—å –∫—ñ–ª—å–∫—ñ—Å—Ç—å –µ–ø–æ—Ö –Ω–∞–≤—á–∞–Ω–Ω—è",
                "‚Ä¢ –î–æ–¥–∞–π—Ç–µ –±—ñ–ª—å—à–µ —Ç–µ—Ö–Ω—ñ—á–Ω–∏—Ö —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä—ñ–≤",
                "‚Ä¢ –°–ø—Ä–æ–±—É–π—Ç–µ –∑–º—ñ–Ω–∏—Ç–∏ learning rate",
                "‚Ä¢ –ü–µ—Ä–µ–≤—ñ—Ä—Ç–µ –∫–æ—Ä–µ–ª—è—Ü—ñ—é –æ–∑–Ω–∞–∫ –∑ —Ü—ñ–ª–ª—é"
            ])
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó —â–æ–¥–æ –æ–∑–Ω–∞–∫
        if 'few_features' in analysis['issues']:
            recommendations.append("‚Ä¢ –î–æ–¥–∞–π—Ç–µ –±—ñ–ª—å—à–µ –æ–∑–Ω–∞–∫ (MA, RSI, Volatility)")
        
        if 'many_features' in analysis['issues']:
            recommendations.append("‚Ä¢ –ó–º–µ–Ω—à—ñ—Ç—å –∫—ñ–ª—å–∫—ñ—Å—Ç—å –æ–∑–Ω–∞–∫ –∞–±–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–π—Ç–µ feature selection")
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó —â–æ–¥–æ –Ω–∞–≤—á–∞–Ω–Ω—è
        if history and len(history.history.get('val_loss', [])) > 10:
            val_loss = history.history['val_loss']
            if min(val_loss) == val_loss[-1]:
                recommendations.append("‚úÖ –ú–æ–¥–µ–ª—å –ø—Ä–æ–¥–æ–≤–∂—É—î –ø–æ–∫—Ä–∞—â—É–≤–∞—Ç–∏—Å—å - –∑–±—ñ–ª—å—à—ñ—Ç—å –∫—ñ–ª—å–∫—ñ—Å—Ç—å –µ–ø–æ—Ö")
        
        # –ó–∞–≥–∞–ª—å–Ω—ñ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó
        if analysis['metrics']['r2'] > 0.7:
            recommendations.extend([
                "‚úÖ –í—ñ–¥–º—ñ–Ω–Ω–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç!",
                "‚Ä¢ –ú–æ–∂–µ—Ç–µ –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—É–≤–∞—Ç–∏ –∑ –±—ñ–ª—å—à —Å–∫–ª–∞–¥–Ω–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏",
                "‚Ä¢ –°–ø—Ä–æ–±—É–π—Ç–µ –¥–æ–¥–∞—Ç–∏ —á–∞—Å–æ–≤—ñ –æ–∑–Ω–∞–∫–∏"
            ])
        
        return recommendations

    
    def create_quality_report(self, history, quality_analysis, symbol):
        """–°—Ç–≤–æ—Ä–µ–Ω–Ω—è –∑–≤—ñ—Ç—É —è–∫–æ—Å—Ç—ñ –Ω–∞–≤—á–∞–Ω–Ω—è"""
        report = {
            'symbol': symbol,
            'timestamp': datetime.now().isoformat(),
            'training_quality': quality_analysis['status'],
            'score': quality_analysis['score'],
            'metrics': quality_analysis['metrics'],
            'issues': quality_analysis['issues'],
            'recommendations': quality_analysis['recommendations'],
            'epochs_trained': len(history.history['loss']) if history else 0,
            'total_epochs': self.epochs_var.get()
        }
        
        # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –∑–≤—ñ—Ç—É
        report_path = f'models/{symbol}_quality_report.json'
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        return report

    def find_optimal_learning_rate(self, model, X_train, y_train, X_val, y_val):
        """–ó–Ω–∞—Ö–æ–¥–∂–µ–Ω–Ω—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ learning rate"""
        learning_rates = [1e-5, 3e-5, 1e-4, 3e-4, 1e-3, 3e-3, 1e-2, 3e-2]
        best_lr = 0.001
        best_loss = float('inf')
        
        self.add_log_message("üîç –ü–æ—à—É–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ Learning Rate...\n")
        
        for lr in learning_rates:
            # –ö–ª–æ–Ω—É—î–º–æ –º–æ–¥–µ–ª—å
            model_clone = tf.keras.models.clone_model(model)
            model_clone.compile(
                optimizer=tf.keras.optimizers.Adam(learning_rate=lr),
                loss='mse',
                metrics=['mae']
            )
            
            # –ö–æ—Ä–æ—Ç–∫–µ –Ω–∞–≤—á–∞–Ω–Ω—è
            history = model_clone.fit(
                X_train, y_train,
                epochs=5,
                batch_size=32,
                validation_data=(X_val, y_val),
                verbose=0
            )
            
            val_loss = history.history['val_loss'][-1]
            self.add_log_message(f"  LR: {lr:.0e} -> Val Loss: {val_loss:.6f}\n")
            
            if val_loss < best_loss:
                best_loss = val_loss
                best_lr = lr
        
        self.add_log_message(f"‚úÖ –û–ø—Ç–∏–º–∞–ª—å–Ω–∏–π Learning Rate: {best_lr:.0e}\n")
        return best_lr

    def analyze_feature_correlation(self, data, target_column='Close'):
        """–ê–Ω–∞–ª—ñ–∑ –∫–æ—Ä–µ–ª—è—Ü—ñ—ó –æ–∑–Ω–∞–∫ –∑ —Ü—ñ–ª—å–æ–≤–æ—é –∑–º—ñ–Ω–Ω–æ—é"""
        correlation = data.corr()[target_column].sort_values(ascending=False)
        
        self.add_log_message("üìä –ö–û–†–ï–õ–Ø–¶–Ü–Ø –û–ó–ù–ê–ö –ó –¶–Ü–ù–û–Æ –ó–ê–ö–†–ò–¢–¢–Ø:\n")
        for feature, corr_value in correlation.items():
            if feature != target_column:
                significance = "üöÄ –í–ò–°–û–ö–ê" if abs(corr_value) > 0.3 else "‚úÖ –ü–û–ú–Ü–†–ù–ê" if abs(corr_value) > 0.1 else "‚ö†Ô∏è –ù–ò–ó–¨–ö–ê"
                self.add_log_message(f"  {feature:25s}: {corr_value:7.3f} ({significance})\n")
        
        return correlation

    def select_features_by_correlation(self, data, min_correlation=0.1, target_column='Close'):
        """–í–∏–±—ñ—Ä –æ–∑–Ω–∞–∫ –Ω–∞ –æ—Å–Ω–æ–≤—ñ –∫–æ—Ä–µ–ª—è—Ü—ñ—ó"""
        correlation = data.corr()[target_column]
        selected_features = correlation[abs(correlation) >= min_correlation].index.tolist()
        
        # –ó–∞–≤–∂–¥–∏ –≤–∫–ª—é—á–∞—î–º–æ —Ü—ñ–ª—å–æ–≤—É –∑–º—ñ–Ω–Ω—É
        if target_column not in selected_features:
            selected_features.append(target_column)
        
        self.add_log_message(f"üìà –í–Ü–î–Ü–ë–†–ê–ù–û {len(selected_features)} –û–ó–ù–ê–ö –ó –ö–û–†–ï–õ–Ø–¶–Ü–Ñ–Æ ‚â• {min_correlation}:\n")
        for feature in selected_features:
            self.add_log_message(f"  ‚Ä¢ {feature}\n")
        
        return selected_features


    
    def show_log_window(self):
        """–ü–æ–∫–∞–∑—É—î –≤—ñ–∫–Ω–æ –ª–æ–≥—É"""
        if self.log_window is None or not self.log_window.winfo_exists():
            self.log_window = tk.Toplevel(self.parent)
            self.log_window.title("–õ–æ–≥ –Ω–∞–≤—á–∞–Ω–Ω—è")
            self.log_window.geometry("800x400")
            self.log_window.protocol("WM_DELETE_WINDOW", self.hide_log_window)
            
            self.log_text = scrolledtext.ScrolledText(self.log_window, wrap=tk.WORD)
            self.log_text.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)
            
            # –ü–µ—Ä–µ–Ω–∞–ø—Ä–∞–≤–ª—è—î–º–æ stdout/stderr
            self.original_stdout = sys.stdout
            self.original_stderr = sys.stderr
            sys.stdout = TextRedirector(self.log_text, "stdout")
            sys.stderr = TextRedirector(self.log_text, "stderr")
    
    def hide_log_window(self):
        """–•–æ–≤–∞—î –≤—ñ–∫–Ω–æ –ª–æ–≥—É"""
        if self.log_window is not None and self.log_window.winfo_exists():
            # –í—ñ–¥–Ω–æ–≤–ª—é—î–º–æ stdout/stderr
            if self.original_stdout:
                sys.stdout = self.original_stdout
            if self.original_stderr:
                sys.stderr = self.original_stderr
            
            self.log_window.destroy()
            self.log_window = None
            self.log_text = None
            self.show_log_var.set(False)
    
    def show_context_menu(self, event):
        """–ü–æ–∫–∞–∑—É—î –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–µ –º–µ–Ω—é"""
        self.context_menu.tk_popup(event.x_root, event.y_root)
        
    def train_models_thread(self, symbols, params):
        """–ü–æ—Ç–æ–∫–æ–≤–∞ —Ñ—É–Ω–∫—Ü—ñ—è –¥–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª–µ–π"""
        successful = 0
        failed = 0
        
        for i, symbol in enumerate(symbols):
            if self.training_stop_flag:
                break
            
            self.status_callback(f"–°—Ç–∞—Ç—É—Å: –ù–∞–≤—á–∞–Ω–Ω—è {symbol} ({i+1}/{len(symbols)})...")
            
            try:
                # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –¥–∞–Ω—ñ
                data_file = self.model_manager.get_data_file(symbol)
                if not data_file:
                    self.status_callback(f"–°—Ç–∞—Ç—É—Å: –§–∞–π–ª –¥–∞–Ω–∏—Ö –¥–ª—è {symbol} –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ")
                    failed += 1
                    continue
                
                df = pd.read_csv(data_file)
                if len(df) < 100:  # –ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∑–∞–ø–∏—Å—ñ–≤
                    self.status_callback(f"–°—Ç–∞—Ç—É—Å: –ù–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ –¥–∞–Ω–∏—Ö –¥–ª—è {symbol} ({len(df)} –∑–∞–ø–∏—Å—ñ–≤)")
                    failed += 1
                    continue
                
                # –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–∏—Ö
                X_train, X_test, y_train, y_test, scaler, feature_names = self.prepare_data(df, params)
                
                if X_train is None or len(X_train) == 0:
                    self.status_callback(f"–°—Ç–∞—Ç—É—Å: –ü–æ–º–∏–ª–∫–∞ –ø—ñ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–∏—Ö –¥–ª—è {symbol}")
                    failed += 1
                    continue
                
                # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ
                model = self.create_model(params, X_train.shape[1:])
                
                # Callbacks
                callbacks = [
                    EarlyStopping(
                        monitor='val_loss',
                        patience=params['patience'],
                        min_delta=params['min_delta'],
                        restore_best_weights=True,
                        verbose=1
                    ),
                    ReduceLROnPlateau(
                        monitor='val_loss',
                        factor=0.5,
                        patience=params['patience'] // 2,
                        min_lr=1e-6,
                        verbose=1
                    ),
                    TrainingProgressCallback(
                        self.status_callback,
                        self.update_progress,
                        params['epochs'],
                        self.log_callback if self.log_text else None
                    )
                ]
                
                # –ù–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ
                history = model.fit(
                    X_train, y_train,
                    epochs=params['epochs'],
                    batch_size=params['batch_size'],
                    validation_split=0.2,
                    callbacks=callbacks,
                    verbose=0
                )
                
                # –û—Ü—ñ–Ω–∫–∞ –º–æ–¥–µ–ª—ñ
                train_loss = model.evaluate(X_train, y_train, verbose=0)
                test_loss = model.evaluate(X_test, y_test, verbose=0)
                
                # –ü—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è
                y_pred = model.predict(X_test, verbose=0)
                
                # –ó–≤–æ—Ä–æ—Ç–Ω—î –º–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è
                y_test_orig = scaler.inverse_transform(
                    np.concatenate([np.zeros((len(y_test), len(feature_names) - 1)), y_test.reshape(-1, 1)], axis=1)
                )[:, -1]
                
                y_pred_orig = scaler.inverse_transform(
                    np.concatenate([np.zeros((len(y_pred), len(feature_names) - 1)), y_pred.reshape(-1, 1)], axis=1)
                )[:, -1]
                
                # –ú–µ—Ç—Ä–∏–∫–∏
                mse = mean_squared_error(y_test_orig, y_pred_orig)
                mae = mean_absolute_error(y_test_orig, y_pred_orig)
                r2 = r2_score(y_test_orig, y_pred_orig)
                
                # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ
                model_info = {
                    'symbol': symbol,
                    'training_type': params['training_type'],
                    'metrics': {
                        'train_loss': float(train_loss),
                        'test_loss': float(test_loss),
                        'mse': float(mse),
                        'mae': float(mae),
                        'r2': float(r2)
                    },
                    'parameters': params,
                    'feature_names': feature_names,
                    'training_date': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                    'data_points': len(df)
                }
                
                self.model_manager.save_model(model, model_info)
                
                # –û–Ω–æ–≤–ª–µ–Ω–Ω—è –≥—Ä–∞—Ñ—ñ–∫–∞
                self.update_training_plot(history, symbol, model_info['metrics'])
                
                successful += 1
                self.status_callback(f"–°—Ç–∞—Ç—É—Å: ‚úÖ {symbol} –Ω–∞–≤—á–µ–Ω–æ —É—Å–ø—ñ—à–Ω–æ (R¬≤={r2:.4f})")
                
            except Exception as e:
                failed += 1
                error_msg = f"–°—Ç–∞—Ç—É—Å: –ü–æ–º–∏–ª–∫–∞ –Ω–∞–≤—á–∞–Ω–Ω—è {symbol}: {str(e)}"
                self.status_callback(error_msg)
                print(f"ERROR: {error_msg}")
                import traceback
                traceback.print_exc()
        
        self.status_callback(f"–°—Ç–∞—Ç—É—Å: ‚úÖ –ù–∞–≤—á–∞–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –£—Å–ø—ñ—à–Ω–æ: {successful}, –ù–µ –≤–¥–∞–ª–æ—Å—è: {failed}")
        self.status_label.config(text="–ù–∞–≤—á–∞–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ", foreground="green")
        self.progress_bar['value'] = 100
        
    def calculate_basic_indicators(self, df):
        """–†–æ–∑—Ä–∞—Ö–æ–≤—É—î –±–∞–∑–æ–≤—ñ —Ç–µ—Ö–Ω—ñ—á–Ω—ñ —ñ–Ω–¥–∏–∫–∞—Ç–æ—Ä–∏"""
        df = df.copy()
        
        # Returns
        if 'Close' in df.columns:
            df['Returns'] = df['Close'].pct_change()
        
        # Moving Averages
        if 'Close' in df.columns:
            df['MA_5'] = df['Close'].rolling(window=5).mean()
            df['MA_20'] = df['Close'].rolling(window=20).mean()
        
        # Volatility
        if 'Returns' in df.columns:
            df['Volatility'] = df['Returns'].rolling(window=20).std()
        
        return df
        
    def log_callback(self, message):
        """Callback –¥–ª—è –ª–æ–≥—É–≤–∞–Ω–Ω—è"""
        if self.log_text:
            self.log_text.insert(tk.END, message + '\n')
            self.log_text.see(tk.END)
    
    def toggle_log_window(self):
        """–ü–µ—Ä–µ–º–∏–∫–∞—á –≤—ñ–∫–Ω–∞ –ª–æ–≥—É"""
        if self.show_log_var.get():
            self.open_log_window()
        else:
            self.close_log_window()

    def open_log_window(self):
        """–í—ñ–¥–∫—Ä–∏—Ç—Ç—è –≤—ñ–∫–Ω–∞ –ª–æ–≥—É –∑ –ø–µ—Ä–µ–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—è–º –≤—Å—ñ—Ö –ª–æ–≥—ñ–≤"""
        if self.log_window and self.log_window.winfo_exists():
            self.log_window.lift()
            return
        
        self.log_window = tk.Toplevel(self.parent)
        self.log_window.title("–õ–æ–≥ –Ω–∞–≤—á–∞–Ω–Ω—è")
        self.log_window.geometry("900x600")
        self.log_window.protocol("WM_DELETE_WINDOW", self.on_log_window_close)
        
        # –ì–æ–ª–æ–≤–Ω–∏–π —Ñ—Ä–µ–π–º
        main_frame = ttk.Frame(self.log_window)
        main_frame.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)
        
        # –¢–µ–∫—Å—Ç–æ–≤–µ –ø–æ–ª–µ –¥–ª—è –ª–æ–≥—É
        log_frame = ttk.LabelFrame(main_frame, text="–õ–æ–≥ –Ω–∞–≤—á–∞–Ω–Ω—è (–≤—Å—ñ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è)")
        log_frame.pack(fill=tk.BOTH, expand=True, pady=(0, 10))
        
        self.log_text = scrolledtext.ScrolledText(log_frame, wrap=tk.WORD, width=100, height=25)
        self.log_text.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)
        
        # –ö–Ω–æ–ø–∫–∏ —É–ø—Ä–∞–≤–ª—ñ–Ω–Ω—è
        button_frame = ttk.Frame(main_frame)
        button_frame.pack(fill=tk.X, pady=5)
        
        ttk.Button(button_frame, text="–ó–±–µ—Ä–µ–≥—Ç–∏ –ª–æ–≥", command=self.save_log).pack(side=tk.LEFT, padx=5)
        ttk.Button(button_frame, text="–û—á–∏—Å—Ç–∏—Ç–∏ –ª–æ–≥", command=self.clear_log).pack(side=tk.LEFT, padx=5)
        ttk.Button(button_frame, text="–ó–∞–∫—Ä–∏—Ç–∏", command=self.close_log_window).pack(side=tk.RIGHT, padx=5)
        
        # –ü–µ—Ä–µ–Ω–∞–ø—Ä–∞–≤–ª—è—î–º–æ –≤—Å—ñ –ª–æ–≥–∏
        self.redirect_all_logs()
        
        # –î–æ–¥–∞—î–º–æ –ø–æ—á–∞—Ç–∫–æ–≤–µ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è
        self.add_log_message("=== –õ–æ–≥ –Ω–∞–≤—á–∞–Ω–Ω—è –Ω–µ–π—Ä–æ–º–µ—Ä–µ–∂ ===\n")
        self.add_log_message(f"–ß–∞—Å –ø–æ—á–∞—Ç–∫—É: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        self.add_log_message("=" * 60 + "\n\n")

    def redirect_all_logs(self):
        """–ü–µ—Ä–µ–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—è –≤—Å—ñ—Ö –ª–æ–≥—ñ–≤ —É –≤—ñ–∫–Ω–æ –ª–æ–≥—É"""
        import logging
        from logging import StreamHandler
        
        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω—ñ –æ–±—Ä–æ–±–Ω–∏–∫–∏
        if not hasattr(self, 'original_handlers'):
            self.original_handlers = {}
        
        # –û—Ç—Ä–∏–º—É—î–º–æ –∫–æ—Ä–µ–Ω–µ–≤–∏–π –ª–æ–≥–µ—Ä
        root_logger = logging.getLogger()
        
        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –ø–æ—Ç–æ—á–Ω—ñ –æ–±—Ä–æ–±–Ω–∏–∫–∏
        self.original_handlers['root'] = root_logger.handlers.copy()
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ —Å–ø–µ—Ü—ñ–∞–ª—å–Ω–∏–π –æ–±—Ä–æ–±–Ω–∏–∫ –¥–ª—è –ø–µ—Ä–µ–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—è
        class LogRedirectHandler(StreamHandler):
            def __init__(self, training_tab):
                super().__init__()
                self.training_tab = training_tab
                
            def emit(self, record):
                try:
                    msg = self.format(record)
                    self.training_tab.add_log_message(msg + '\n')
                except Exception:
                    self.handleError(record)
        
        # –î–æ–¥–∞—î–º–æ –Ω–∞—à –æ–±—Ä–æ–±–Ω–∏–∫ –¥–æ –∫–æ—Ä–µ–Ω–µ–≤–æ–≥–æ –ª–æ–≥–µ—Ä–∞
        redirect_handler = LogRedirectHandler(self)
        redirect_handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))
        root_logger.addHandler(redirect_handler)
        
        # –¢–∞–∫–æ–∂ –ø–µ—Ä–µ–Ω–∞–ø—Ä–∞–≤–ª—è—î–º–æ stdout/stderr
        self.redirect_console_output()
    
    def on_log_window_close(self):
        """–û–±—Ä–æ–±–Ω–∏–∫ –∑–∞–∫—Ä–∏—Ç—Ç—è –≤—ñ–∫–Ω–∞ –ª–æ–≥—É"""
        self.show_log_var.set(False)
        self.close_log_window()

    def close_log_window(self):
        """–ó–∞–∫—Ä–∏—Ç—Ç—è –≤—ñ–∫–Ω–∞ –ª–æ–≥—É –∑ –≤—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è–º –ª–æ–≥—ñ–≤"""
        if self.log_window:
            try:
                # –í—ñ–¥–Ω–æ–≤–ª—é—î–º–æ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω—ñ –æ–±—Ä–æ–±–Ω–∏–∫–∏ –ª–æ–≥—ñ–≤
                self.restore_log_handlers()
                
                # –í—ñ–¥–Ω–æ–≤–ª—é—î–º–æ stdout/stderr
                self.restore_console_output()
                
                self.log_window.destroy()
                self.log_window = None
                self.log_text = None
                
                # –û–Ω–æ–≤–ª—é—î–º–æ —Å—Ç–∞–Ω —á–µ–∫–±–æ–∫—Å—É
                self.show_log_var.set(False)
                
            except Exception as e:
                print(f"–ü–æ–º–∏–ª–∫–∞ –∑–∞–∫—Ä–∏—Ç—Ç—è –≤—ñ–∫–Ω–∞ –ª–æ–≥—É: {e}")

    def restore_log_handlers(self):
        """–í—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∏—Ö –æ–±—Ä–æ–±–Ω–∏–∫—ñ–≤ –ª–æ–≥—ñ–≤"""
        if hasattr(self, 'original_handlers'):
            import logging
            root_logger = logging.getLogger()
            
            # –í–∏–¥–∞–ª—è—î–º–æ –≤—Å—ñ –ø–æ—Ç–æ—á–Ω—ñ –æ–±—Ä–æ–±–Ω–∏–∫–∏
            for handler in root_logger.handlers[:]:
                root_logger.removeHandler(handler)
            
            # –í—ñ–¥–Ω–æ–≤–ª—é—î–º–æ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω—ñ –æ–±—Ä–æ–±–Ω–∏–∫–∏
            for handler in self.original_handlers.get('root', []):
                root_logger.addHandler(handler)
    
    def redirect_console_output(self):
        """–ü–µ—Ä–µ–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—è –≤–∏–≤–æ–¥—É –∫–æ–Ω—Å–æ–ª—ñ —É –≤—ñ–∫–Ω–æ –ª–æ–≥—É"""
        if self.original_stdout is None:
            self.original_stdout = sys.stdout
            self.original_stderr = sys.stderr
        
        sys.stdout = self
        sys.stderr = self

    def restore_console_output(self):
        """–í—ñ–¥–Ω–æ–≤–ª–µ–Ω–Ω—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ –≤–∏–≤–æ–¥—É –∫–æ–Ω—Å–æ–ª—ñ"""
        if self.original_stdout:
            sys.stdout = self.original_stdout
            sys.stderr = self.original_stderr
            self.original_stdout = None
            self.original_stderr = None

    def write(self, message):
        """–ó–∞–ø–∏—Å –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è —É –ª–æ–≥ (–¥–ª—è –ø–µ—Ä–µ–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—è stdout/stderr)"""
        if self.log_text and self.log_window and self.log_window.winfo_exists():
            self.add_log_message(message)
        
        # –¢–∞–∫–æ–∂ –≤–∏–≤–æ–¥–∏–º–æ —É –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∏–π stdout –¥–ª—è –∫–æ–Ω—Å–æ–ª—ñ
        if self.original_stdout:
            self.original_stdout.write(message)

    def flush(self):
        """Flush –º–µ—Ç–æ–¥ –¥–ª—è —Å—É–º—ñ—Å–Ω–æ—Å—Ç—ñ –∑ sys.stdout"""
        if self.original_stdout:
            self.original_stdout.flush()

    def add_log_message(self, message):
        """–î–æ–¥–∞–≤–∞–Ω–Ω—è –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è —É –ª–æ–≥ –∑ —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—î—é –∑–∞–π–≤–∏—Ö –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω—å"""
        if self.log_text and self.log_window and self.log_window.winfo_exists():
            try:
                # –§—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è –∑–∞–π–≤–∏—Ö –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω—å
                skip_messages = [
                    "This TensorFlow binary is optimized to use available CPU instructions",
                    "Do not pass an `input_shape`/`input_dim` argument to a layer",
                    "The `save_format` argument is deprecated",
                    "You are saving your model as an HDF5 file"
                ]
                
                if any(skip_msg in message for skip_msg in skip_messages):
                    return
                    
                # –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–µ –ø—Ä–æ–∫—Ä—É—á—É–≤–∞–Ω–Ω—è –¥–æ –∫—ñ–Ω—Ü—è
                self.log_text.insert(tk.END, message)
                self.log_text.see(tk.END)
                self.log_text.update_idletasks()
            except Exception as e:
                print(f"–ü–æ–º–∏–ª–∫–∞ –¥–æ–¥–∞–≤–∞–Ω–Ω—è –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è —É –ª–æ–≥: {e}")

    def save_log(self):
        """–ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –ª–æ–≥—É —É —Ñ–∞–π–ª"""
        if not self.log_text:
            return
        
        try:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"training_log_{timestamp}.txt"
            
            log_content = self.log_text.get(1.0, tk.END)
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(log_content)
            
            self.add_log_message(f"\n‚úÖ –õ–æ–≥ –∑–±–µ—Ä–µ–∂–µ–Ω–æ —É —Ñ–∞–π–ª: {filename}\n")
            messagebox.showinfo("–£—Å–ø—ñ—Ö", f"–õ–æ–≥ –∑–±–µ—Ä–µ–∂–µ–Ω–æ —É —Ñ–∞–π–ª: {filename}")
            
        except Exception as e:
            error_msg = f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –ª–æ–≥—É: {str(e)}"
            self.add_log_message(f"\n{error_msg}\n")
            messagebox.showerror("–ü–æ–º–∏–ª–∫–∞", error_msg)

    def clear_log(self):
        """–û—á–∏—â–µ–Ω–Ω—è –≤–º—ñ—Å—Ç—É –ª–æ–≥—É"""
        if self.log_text:
            if messagebox.askyesno("–ü—ñ–¥—Ç–≤–µ—Ä–¥–∂–µ–Ω–Ω—è", "–û—á–∏—Å—Ç–∏—Ç–∏ –≤–µ—Å—å –ª–æ–≥?"):
                self.log_text.delete(1.0, tk.END)
                self.add_log_message("=== –õ–æ–≥ –æ—á–∏—â–µ–Ω–æ ===\n")
                self.add_log_message(f"–ß–∞—Å: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                self.add_log_message("=" * 50 + "\n\n")
    
    def analyze_training_results(self, history, X_test, y_test, model):
        """–î–µ—Ç–∞–ª—å–Ω–∏–π –∞–Ω–∞–ª—ñ–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –Ω–∞–≤—á–∞–Ω–Ω—è"""
        results = {}
        
        # –ü—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è
        predictions = model.predict(X_test, verbose=0)
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—è –≤ Python —Ç–∏–ø–∏
        def to_python_type(value):
            if hasattr(value, 'item'):
                return value.item()
            return value

        # –û—Å–Ω–æ–≤–Ω—ñ –º–µ—Ç—Ä–∏–∫–∏
        results['mse'] = to_python_type(mean_squared_error(y_test, predictions))
        results['mae'] = to_python_type(mean_absolute_error(y_test, predictions))
        results['r2'] = to_python_type(r2_score(y_test, predictions))
        
        # MAPE
        try:
            results['mape'] = to_python_type(mean_absolute_percentage_error(y_test, predictions))
        except:
            try:
                y_test_clean = y_test[y_test != 0]  # –£–Ω–∏–∫–∞—î–º–æ –¥—ñ–ª–µ–Ω–Ω—è –Ω–∞ –Ω—É–ª—å
                predictions_clean = predictions.flatten()[y_test != 0]
                mape = np.mean(np.abs((y_test_clean - predictions_clean) / y_test_clean)) * 100
                results['mape'] = to_python_type(mape)
            except:
                results['mape'] = float('nan')

        # –ê–Ω–∞–ª—ñ–∑ –∑–∞–ª–∏—à–∫—ñ–≤
        residuals = y_test - predictions.flatten()
        results['residual_mean'] = to_python_type(np.mean(residuals))
        results['residual_std'] = to_python_type(np.std(residuals))
        results['residual_skew'] = to_python_type(pd.Series(residuals).skew())
        results['residual_kurtosis'] = to_python_type(pd.Series(residuals).kurtosis())
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –Ω–∞–≤—á–∞–Ω–Ω—è
        results['final_train_loss'] = to_python_type(history.history['loss'][-1])
        
        if 'val_loss' in history.history:
            results['final_val_loss'] = to_python_type(history.history['val_loss'][-1])
            results['best_val_loss'] = to_python_type(np.min(history.history['val_loss']))
            results['best_epoch'] = to_python_type(np.argmin(history.history['val_loss']) + 1)
            
            # –ê–Ω–∞–ª—ñ–∑ –ø–µ—Ä–µ–Ω–∞–≤—á–∞–Ω–Ω—è
            results['overfitting_ratio'] = to_python_type(results['final_val_loss'] / results['final_train_loss'])
        
        # –ß–∞—Å –Ω–∞–≤—á–∞–Ω–Ω—è (–ø—Ä–∏–±–ª–∏–∑–Ω–æ)
        results['total_epochs'] = to_python_type(len(history.history['loss']))
        
        return results

    def generate_training_report(self, symbol, history, metrics, params):
        """–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –∑–≤—ñ—Ç—É –ø—Ä–æ –Ω–∞–≤—á–∞–Ω–Ω—è"""
        report = {
            'symbol': symbol,
            'training_date': datetime.now().isoformat(),
            'training_type': self.convert_to_serializable(params.get('training_type', 'basic')),
            'parameters': self.convert_to_serializable(params),
            'metrics': self.convert_to_serializable(metrics),
            'training_history': self.convert_to_serializable({
                'final_loss': history.history['loss'][-1],
                'final_val_loss': history.history['val_loss'][-1] if 'val_loss' in history.history else None,
                'best_epoch': np.argmin(history.history['val_loss']) + 1 if 'val_loss' in history.history else len(history.history['loss']),
                'total_epochs': len(history.history['loss'])
            })
        }
        
        # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –∑–≤—ñ—Ç—É
        report_path = f'models/{symbol}_report.json'
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        return report

    def convert_to_serializable(self, obj):
        """–ö–æ–Ω–≤–µ—Ä—Ç—É—î –æ–±'—î–∫—Ç —É JSON-—Å–µ—Ä—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω–∏–π —Ñ–æ—Ä–º–∞—Ç"""
        if isinstance(obj, (np.integer, np.int64, np.int32, np.int8)):
            return int(obj)
        elif isinstance(obj, (np.floating, np.float64, np.float32, np.float16)):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, np.bool_):
            return bool(obj)
        elif isinstance(obj, pd.Timestamp):
            return obj.isoformat()
        elif isinstance(obj, dict):
            return {key: self.convert_to_serializable(value) for key, value in obj.items()}
        elif isinstance(obj, list):
            return [self.convert_to_serializable(item) for item in obj]
        elif isinstance(obj, tuple):
            return tuple(self.convert_to_serializable(item) for item in obj)
        elif hasattr(obj, 'item'):
            return obj.item()
        else:
            return obj

    def analyze_features(self, feature_names):
        """–ê–Ω–∞–ª—ñ–∑ —Ç–∞ –≥—Ä—É–ø—É–≤–∞–Ω–Ω—è –æ–∑–Ω–∞–∫ –¥–ª—è –ª–æ–≥—É–≤–∞–Ω–Ω—è"""
        groups = {
            '–¶—ñ–Ω–æ–≤—ñ': ['Close', 'Open', 'High', 'Low', 'Price'],
            '–í–æ–ª–∞—Ç–∏–ª—å–Ω—ñ—Å—Ç—å': ['Volatility', 'ATR', 'VaR', 'Drawdown', 'Std'],
            'Momentum': ['RSI', 'MACD', 'Stochastic', 'Returns', 'Momentum'],
            '–¢—Ä–µ–Ω–¥–æ–≤—ñ': ['MA', 'EMA', 'SMA', 'Trend'],
            '–û–±\'—î–º': ['Volume', 'OBV'],
            '–ß–∞—Å–æ–≤—ñ': ['Hour', 'Day', 'Month', 'Week', 'Session'],
            '–†–∏–Ω–∫–æ–≤—ñ': ['BTC', 'Market', 'Correlation'],
            '–†–∏–∑–∏–∫': ['Risk', 'Sharpe', 'Sortino', 'VaR', 'ES']
        }
        
        result = {}
        for feature in feature_names:
            found_group = '–Ü–Ω—à—ñ'
            for group_name, keywords in groups.items():
                if any(keyword in feature for keyword in keywords):
                    found_group = group_name
                    break
            
            if found_group not in result:
                result[found_group] = []
            result[found_group].append(feature)
        
        return result
     
    def update_selected_count(self):
        """–û–Ω–æ–≤–∏—Ç–∏ –ª—ñ—á–∏–ª—å–Ω–∏–∫ –æ–±—Ä–∞–Ω–∏—Ö –µ–ª–µ–º–µ–Ω—Ç—ñ–≤"""
        selected_count = len(self.data_tree.selection())
        self.selected_count_var.set(f"–û–±—Ä–∞–Ω–æ: {selected_count}")
       
    
    
    def update_info_text(self, text):
        """–û–Ω–æ–≤–ª–µ–Ω–Ω—è —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ–π–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç—É"""
        self.info_text.insert(tk.END, text + "\n")
        self.info_text.see(tk.END)
        
    def safe_status_callback(self, message):
        """–ë–µ–∑–ø–µ—á–Ω–∏–π –≤–∏–∫–ª–∏–∫ —Å—Ç–∞—Ç—É—Å callback"""
        if self.status_callback:
            self.status_callback(message)
    
    def safe_progress_callback(self, value):
        """–ë–µ–∑–ø–µ—á–Ω–∏–π –≤–∏–∫–ª–∏–∫ –ø—Ä–æ–≥—Ä–µ—Å callback"""
        if self.progress_callback:
            self.progress_callback(value)

class TextRedirector:
    """–ö–ª–∞—Å –¥–ª—è –ø–µ—Ä–µ–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—è stdout/stderr —É —Ç–µ–∫—Å—Ç–æ–≤–µ –ø–æ–ª–µ"""
    def __init__(self, text_widget, tag="stdout"):
        self.text_widget = text_widget
        self.tag = tag
    
    def write(self, string):
        self.text_widget.insert(tk.END, string, (self.tag,))
        self.text_widget.see(tk.END)
    
    def flush(self):
        pass



